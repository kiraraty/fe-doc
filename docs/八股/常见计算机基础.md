## 编码和加密

### 1.摘要算法 md5等

[摘要和加密](https://juejin.cn/post/6844903561478799368)

MD5算法就是一种常见的摘要算法。该算法可以生成压缩包的一个128 bit 的二进制串。除了压缩包，也可以应用于其他文件和字符串。比如数据库中不会直接存储账号密码，比如我就习惯将 密码拼接一个无规律的字符串 然后计算出MD5摘要放入数据库中

摘要

-   摘要是哈希值，我们通过散列算法比如MD5算法就可以得到这个哈希值。
-   摘要只是用于验证数据完整性和唯一性的哈希值，不管原始数据是什么样的，得到的哈希值都是固定长度的。
-   不管原始数据是什么样的，得到的哈希值都是固定长度的，也就是说摘要并不是原始数据加密后的密文，只是一个验证身份的令牌。所以我们无法通过摘要解密得到原始数据。

加密

加密是通过 “加密算法” 将 "明文" 加密成 “密文”。 我们可以通过 “密钥” 和 “解密算法” 将 “密文” 还原成 “明文”。



### 2.base64编码

[Base64原理](https://juejin.cn/post/6844903698045370376)

我们的图片大部分都是可以转换成base64编码的data：image。 这个在将canvas保存为img的时候尤其有用。现代浏览器都已经支持原生的基于base64的encode和decode，例如**btoa**和**atob**

看一下Base64的索引表，字符选用了"A-Z、a-z、0-9、+、/" 64个可打印字符。数值代表字符的索引，这个是标准Base64协议规定的，不能更改。64个字符用6个bit位就可以全部表示，一个字节有8个bit位，剩下两个bit就浪费掉了，这样就不得不牺牲一部分空间了。这里需要弄明白的就是一个Base64字符是8个bit，但是有效部分只有右边的6个bit，左边两个永远是0。

那么怎么用6个有效bit来表示传统字符的8个bit呢？8和6的最小公倍数是24，也就是说3个传统字节可以由4个Base64字符来表示，保证有效位数是一样的，这样就多了1/3的字节数来弥补Base64只有6个有效bit的不足。你也可以说用两个Base64字符也能表示一个传统字符，但是采用最小公倍数的方案其实是最减少浪费的。结合下边的图比较容易理解。Man是三个字符，一共24个有效bit，只好用4个Base64字符来凑齐24个有效位。红框表示的是对应的Base64，6个有效位转化成相应的索引值再对应Base64字符表，查出"Man"对应的Base64字符是"TWFU"。说到这里有个原则不知道你发现了没有，要转换成Base64的**最小单位就是三个字节**，对一个字符串来说每次都是三个字节三个字节的转换，对应的是Base64的四个字节。这个搞清楚了其实就差不多了。

但是转换到最后你发现不够三个字节了怎么办呢？愿望终于实现了，我们可以用两个Base64来表示一个字符或用三个Base64表示两个字符，像下图的A对应的第二个Base64的二进制位只有两个，把后边的四个补0就是了。所以A对应的Base64字符就是QQ。上边已经说过了，原则是Base64字符的**最小单位是四个字符一组**，那这才两个字符，后边补两个"="吧。其实不用"="也不耽误解码，之所以用"="，可能是考虑到多段编码后的Base64字符串拼起来也不会引起混淆。由此可见Base64字符串只可能最后出现一个或两个"="，中间是不可能出现"="的。下图中字符"BC"的编码过程也是一样的。

##### 总结

　　说起Base64编码可能有些奇怪，因为大多数的编码都是由字符转化成二进制的过程，而从二进制转成字符的过程称为解码。而Base64的概念就恰好反了，由二进制转到字符称为编码，由字符到二进制称为解码。

　　Base64编码主要用在传输、存储、表示二进制等领域，还可以用来加密，但是这种加密比较简单，只是一眼看上去不知道什么内容罢了，当然也可以对Base64的字符序列进行定制来进行加密。

　　Base64编码是从二进制到字符的过程，像一些中文字符用不同的编码转为二进制时，产生的二进制是不一样的，所以最终产生的Base64字符也不一样。例如"上网"对应utf-8格式的Base64编码是"5LiK572R"，对应GB2312格式的Base64编码是"yc/N+A=="。

### 3.字符编码理论

 [从字符、字符集、字符编码一路到URLEncode](https://juejin.cn/post/6913866364243738631#heading-12)

 [一文弄懂字符编码和应用](https://juejin.cn/post/7042899099879899173)

#### 1. 字符集(Character set)

字符类似a,b，你，&等各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字，emoji等是一个字符。而**字符的集合就称之为字符集**。我们常见字符集有ASCCII字符集、Unicode字符集、GBK字符集等。

**ASCCII字符集** 主要包括控制字符（回车键、退格、换行键等）；可显示字符（英文大小写字符、阿拉伯 数字和西文符号）。

**Unicode字符集** ASCCII具有很大的局限性，只能表示至多127个字符，而诸如汉字，阿拉伯语等都有自己的文字字符，ASCCII字符集是不能表示的。Unicode字符集可以简单的理解为通用所有字符的一个集合。

每个字符对应一个数字，每个数字对应一个字符，而这个数字也称之为**码点**。根据码点就可以字符集中索引到对应的字符。

#### 2. 字符编码

计算机系统中所有的数据都是用二进制进行传输和存储的，当然字符也不例外。把一个数值与字符集中的字符进行匹配建立一一对应关系的规则称之为字符编码。跟我们差字典一样，同一本字典(编码)第几页第几个字(字符)就能确定唯一一个字符。通常来讲，每一个字符在字符集中都有一个码点，我们对根据码点就能确认一个字符，我们对码点就行储存和编码就能达到对字符进行传输和存储的目的。

##### 2.1. **UTF-8**

>   UTF-8（8-bit Unicode Transformation Format）是一种针对Unicode的可变长度字符编码，也是一种前缀码。它可以用来表示Unicode标准中的任何字符，且其编码中的第一个字节仍与ASCII兼容。

UTF-8是一种变长的字符编码，一个字符的UTF-8编码可能占1-4个字节，而占某个字符具体占几个字节取决，字符码点的值。

具体表示的时候，

-   如果第一个字节的第一bit为0，表示当前字符只用一个字节就可以表示。具体的码点值，就是当前字节除0位之外其他7个字节。
-   如果第一个字节的第一个bit不为0，则从开头知道碰到第一个0的过程中有几个1，就用几个字节表示。好像描述的不太好，看图更明了。如下图

```
Unicode符号范围(码点范围)     |       UTF-8编码模板
(十六进制)                   |       （二进制）
---------------------------+---------------------------------------------
0000 0000-0000 007F        | 0xxxxxxx                             |可用7bit
0000 0080-0000 07FF        | 110xxxxx 10xxxxxx					  |可用11bit
0000 0800-0000 FFFF        | 1110xxxx 10xxxxxx 10xxxxxx           |可用16bit
0001 0000-0010 FFFF        | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx  |可用21bit
```

上表表示如何从一个从Unicode 转化到UTF-8

对于任意字符，UInicode码点的值，在上表中找到对应的编码模板，然后把码点值转换为2进制，依次从后往前填入码位(上表中的x)，不足的用0补齐。

![image-20220627161643818](https://s2.loli.net/2022/06/27/6KkVOiyDsIERQB8.png)

比如 :[**你**] 这个字的Unicode编码是0x4f60。0x6C49在0x0800-0xFFFF之间，使用3字节模板：1110xxxx 10xxxxxx 10xxxxxx。将ox4f60 写成二进制是： 1001111 01100000 ， **从这个二进制值从低到高取出比特，从模板的低位到高位替换模板中的x的位置**，不足的用0补齐，得到：11100100 10111101 10100000。可见[你]这个字符utf-8编码占3个字节

#### 3. URL

##### URi和URL和URN之间的关系

###### **URi**

uri的全称为：uniform resource identifier，中文为统一资源标识符，用来表示唯一的标识资源。web上可用的每一种资源,例如html文档，图像，视频片段，程序等都是由一个通用资源标识符进行定位。

uri组成

>1、访问资源的命名机制 2、存放资源的主机名 3、资源自身的名称，由路由表示 
>
>我们以http://www.baidu.com/index.html为例： 
>
>这个uri是这样的：这是一个http协议访问的资源，位于www.baidu.com主机上，通过路径/index.html来进行访问。

绝对uri

>uri存在绝对uri和相对uri之分，绝对uri指的是scheme(后面跟着冒号)开头的uri。前面的http://www.baidu.com/index.html就是一个绝对的uri。还存在其他的例子：mailto:jeff@javajeff.com.等等到时绝对的uri.

相对的uri

>相对的uri不含任何命名规则。它的路径通查指同一台机器上的资源。

`例如：<img src="../icons/logo.png">就是相对的uri。`

在html中，uri被用来 1、链接到另一个文档或资源 2、链接到一个外部样式表或者脚本 3、在页内包含图形等

###### URL

url是uniform resource locator，指的是统一资源名称，它是一种具体的uri，即url可以用来标识一个资源。

>1、internet资源类型，指出www客户程序用来操作的工具。
>2、服务器地址(host)：指出www网页所在的服务器域名。必需的。
>3、端口（port）：有时（并非总是这样），对某些资源的访问来说，需给出相应的服务器提供端口。可选的。
>4、路径：指明服务器上某资源的位置。与端口一样，路径并非总是需要的，可选。

###### URN

>URN:uniform resource name,统一资源命名，是通过名字来标识，比如`mailto:java-net@java.sun.com`
>URN可以理解为通过名称来标识位置。但是其流行还需要假以时日，因为他需要更精密软件支持。

###### 关系

![image-20220627162623765](https://s2.loli.net/2022/06/27/eLispZYh3FRDxAa.png)

uri是统一资源标识符，用来唯一确定资源，他是一种抽象，也就是说不管用什么方法，只要可以唯一确定资源，那么那就是uri。 

url:统一资源定位符，urn：统一资源名称。 **url和urn是uri的子集**，url可以理解为用地址来定位资源，urn可以理解为用名称来定位资源。

##### 3.1. URL格式

统一资源定位符的完整格式如下：

```javascript
        foo://example.com:8042/over/there?name=ferret#nose
        \_/   \______________/\_________/ \_________/ \__/
         |           |            |            |        |
      scheme     authority       path        query   fragment
         |   _____________________|__
        / \ /                        \
        urn:example:animal:ferret:nose
```

一个完整的URL主要有以下部分组成这些部分被URL的保留字符相连。

-   协议（sheme）。
-   层级URL标记符号，为[//],是固定不变的
-   访问资源需要的凭证信息(authority）
-   服务器地址(host)。通常为域名，有时为IP地址
-   端口号(port)。以数字方式表示，若为默认值可省略
-   路径(path)。以“/”字符区别路径中的每一个路径名称
-   查询(query)。键值对参数，以“?”字符为起点，每个参数键值对以“&”隔开，再以“=”分开参数名称和值
-   片段。以“#”字符为起点

##### 3.2. URL中的字符

-   RFC中规定，URL中只有可显的asscii字符集中的字符是合法的。

-   URL的字符中有特殊的含义通常用来做分隔符如`www.google.com?a=12&b=34` 中`:/?&`都为url中的**保留字符**。

    **2005年规定的保留字符:**

```java
  !  *   '   (   ) ; :  @  &  =  +  $  ,   ?  #  [  ]
```

-   URL字符中,大写字母（A-Z)、小写字母(a-z)、数字(0-9)等为**非保留字符**。这些字符含义只是表示字面量在协议中不允许特殊的语义。

    **2005年规定的非保留字符:**

```javascript
A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z 0 1 2 3 4 5 6 7 8 9 - _ . ~
```

-   URL字符中除了保留字符和非保留字符之外的字符，比如百分号符(%),这些**其他字符**，允许有一些特殊的语义但是不强制。

#### 4. URL encoding 百分号编码

##### 4.1. 为什么要URL encoding

-   在具体的URL中诸如path或者query部分，如果存在着保留字段会有语义上的不合法性。 比如：`http://www.abac.com/a/?abc=1&2&ef=24`。&为保留字符，用以链接两个query键值对。而在这个URL中想要传递键为abc 值为1&2的时候在语法上是不合法的。
-   URL的字符集只是ASCII字符集的一个字集，如果要传递URL合法的字符以外的字符，是没办法表示的。比如要在url的path中有"好"这个字符。
-   另，URL字符集中的其他字符集有可能存在不安全的字符。是有一些特殊语义的。

基于以上三点,我们需要一种编码来解决URl使用过程中的一些歧义。来解决这个编码问题的就是URLEncode。

##### 4.2. URL encoding 的规则

>   2005年1月发布的RFC 3986

对可能会引起歧义的保留字符、不在ASCII码之外的字符和URL字符集范围内的其他字符进行URlEncode，以保证URL语义的正确性。URLEncode的规则：

-   未保留字符不需要百分号编码。比如字符a 百分号编码后之后还是a。
-   对保留字符的百分号编码。编码逻辑为：对应字符的ASCII的值表示为两个16进制的值，然后在其前面放置%字符。比如，保留字符？对应的ASCII码点值为6（十进制）对应16进制为3F，然后在16进制值前面加上%字符最终得到保留字符?的url编码为%3F。注意:对字节百分号编码后里16进制的表示都为大写字母比如%3F,且在URL编码后默认不区分大小写。
-   对于其他字符百分号编码。编码逻辑为:把对应字符用某种**编码格式(**如utf-8)转为字节序列，然后把每个字节序列的值进行百分号编码。这里展开说一下，所谓的对每个字节进行编码，把每个字节的值转为16进制，然后再16进制前加百分号就得到了对应的百分号编码。

将需要转码的字符转为16进制，然后从右到左，取4位(不足4位直接处理)，每2位做一位，前面加上%，编码成%XY格式。

比如：

空格 ASCII码是32，对应16进制是20，那么urlencode编码结果是:%20,但在新标准中空格对应的是+,见RFC-1738

比如：

中 ASCII码是-10544，对应的16进制是D6D0，那么urlencode编码结果是:%D6%D0

##### 4.3. URL encoding 用途

-   http post形式发送时指定Content-Type为 `application/x-www-form-urlencode`时，postbody中的键值对都会进行百分号编码以保证数据的无歧义。

-   当我们的http url中，query和path部分的内容如果需要被urlencode也要进行百分号编码，不做赘述。

    

### 4.常见加密算法

[浅谈常见的七种加密算法及实现](https://juejin.cn/post/6844903638117122056)

`MD5` 用的是 **哈希函数**，它的典型应用是对一段信息产生 **信息摘要**，以 **防止被篡改**。严格来说，`MD5` 不是一种 **加密算法** 而是 **摘要算法**。无论是多长的输入，`MD5` 都会输出长度为 `128bits` 的一个串 (通常用 `16` **进制** 表示为 `32` 个字符)。

`SHA1` 是和 `MD5` 一样流行的 **消息摘要算法**，然而 `SHA1` 比 `MD5` 的 **安全性更强**。对于长度小于 `2 ^ 64` 位的消息，`SHA1` 会产生一个 `160` 位的 **消息摘要**。基于 `MD5`、`SHA1` 的信息摘要特性以及 **不可逆** (一般而言)，可以被应用在检查 **文件完整性** 以及 **数字签名** 等场景。

`HMAC` 是密钥相关的 **哈希运算消息认证码**（Hash-based Message Authentication Code），`HMAC` 运算利用 **哈希算法** (`MD5`、`SHA1` 等)，以 **一个密钥** 和 **一个消息** 为输入，生成一个 **消息摘要** 作为 **输出**。

`HMAC` **发送方** 和 **接收方** 都有的 `key` 进行计算，而没有这把 `key` 的第三方，则是 **无法计算** 出正确的 **散列值**的，这样就可以 **防止数据被篡改**。

`AES`、`DES`、`3DES` 都是 **对称** 的 **块加密算法**，**加解密** 的过程是 **可逆的**。常用的有 `AES128`、`AES192`、`AES256` (默认安装的 `JDK` 尚不支持 `AES256`，需要安装对应的 `jce` 补丁进行升级 `jce1.7`，`jce1.8`)。

`RSA` 加密算法是目前最有影响力的 **公钥加密算法**，并且被普遍认为是目前 **最优秀的公钥方案** 之一。`RSA` 是第一个能同时用于 **加密** 和 **数字签名** 的算法，它能够 **抵抗** 到目前为止已知的 **所有密码攻击**，已被 `ISO` 推荐为公钥数据加密标准。

### 5.登录密码加密

Hash特点：
 1.算法是公开的
 2.对相同数据运算,得到的结果是一样的
 3.对不同数据运算,如MD5得到的结果默认是128位,32个字符（16进制标识）。
 4.没法逆运算
 5.信息摘要，信息“指纹”，是用来做数据识别的。

hash加密：
 1.md5. 2.sha1  2.sha256  3.sha516

用户密码加密
 1.首先密码加密不能用rsa加密，因为rsa加密虽然在传输过程中是相对安全的，但是一旦开发人员变动带走了私钥与密钥，是很容易的获取用户的登录密码

2.hash加密，hash加密是不可逆的，加密后传输至服务端后，开发人员是获取不出用户的登录密码的，只需要将加密的hash值存入数据库中，但是这个方法现在也是不安全的。[https://www.cmd5.com](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.cmd5.com)，这个网址可以查出大部分的密码

3.hash加盐：所谓的hash加盐就是在原密码的基础上，添加上一段随机字符串（该字符串是由客户端开发人员写死的），然后进行hash加密，但是这种方法对于开发者的依赖比较大还有就是灵活性不高，当用户量大了后，如果想更换盐成本会很大

4.hamc加密：也可以说是动态加盐（一个用户一个盐），当用户注册的时候，服务端会返回给客户端一个盐，然后进行两次hash加密，传输给服务端。这样不会对开发人员产生依赖，且由于是一个用户一个盐，这样更换起来也会比较方便



### 6.前后端鉴权

[傻傻分不清之 Cookie、Session、Token、JWT](https://juejin.cn/post/6844904034181070861)

#### Token

token 即使是在计算机领域中也有不同的定义，这里我们说的token，是指**访问资源的凭据**。例如当你调用Google API，需要带上有效 token 来表明你请求的合法性。这个 token 是 Google 给你的，这代表 Google 给你的授权使得你有能力访问 API 背后的资源。

请求 API 时携带 token 的方式也有很多种，通过 HTTP Header 或者 url 参数 或者 google 提供的类库都可以：

```js
// HTTP Header:
GET /drive/v2/files HTTP/1.1
Authorization: Bearer <token>
Host: www.googleapis.com/

// URL query string parameter
GET https://www.googleapis.com/drive/v2/files?token=<token>

// Python:
from googleapiclient.discovery import build
drive = build('drive', 'v2', credentials=credentials)

```

更具体的说，上面用于调用 API 的 token 我们称为细分为 access token。通常 access token 是有有效期限的，如果过期就需要重新获取。那么如何重新获取？现在我们要让时光倒流一会，回顾第一次获取 token 的流程是怎样的:

1.  首先你需要向 Google API 注册你的应用程序，注册完毕之后你会拿到认证信息（credentials）包括 ID 和 secret。不是所有的程序类型都有 secret。
2.  接下来就要向 Google 请求 access token。这里我们先忽略一些细节，例如请求参数（当然需要上面申请到的 secret）以及不同类型的程序的请求方式等。重要的是，如果你想访问的是用户资源，这里就会提醒用户进行授权。
3.  如果用户授权完毕。Google 就会返回 access token。又或者是返回授权代码（authorization code），你再通过代码取得 access token
4.  token 获取到之后，就能够带上 token 访问 API 了

注意在第三步通过 code 兑换 access token 的过程中，Google 并不会仅仅返回 access token，还会返回额外的信息，这其中和之后更新相关的就是 refresh token

一旦 access token 过期，你就可以通过 refresh token 再次请求 access token。

以上只是大致的流程，并且故意省略了一些额外的概念。比如更新 access token 当然也可以不需要 refresh token，这要根据你的请求方式和访问的资源类型而定。



#### 单点登录 SSO (Single sign-on)

通常公司内部会有非常多的工具平台供大家使用，比如人力资源，代码管理，日志监控，预算申请等等。如果每一个平台都实现自己的用户体系的话无疑是巨大的浪费，所以公司内部会有一套公用的用户体系，用户只要登陆之后，就能够访问所有的系统。这就是**单点登录（SSO: Single Sign-On）**

##### 实例

最初的时候，服务的提供者只做了一个单系统，所有的功能都在单系统上，此时不需要`SSO`，一次登录就可以访问所有功能，后来用户量越来越大且功能服务越来越多，为了合理利用资源和降低耦合性，服务商将功能划分为多个子系统，而子系统的用户登录凭证是相互隔离的，如果在这个子系统登录完成，再访问另一个子系统还需要登录，这显然不太合适，而`SSO`就是对于这种问题的解决方案，在多个系统中，用户只需要某一个系统中登录，在其他系统中都无需再次验证用户身份即可静默登录，例如在百度一次登录，再访问贴吧、网盘等都可以静默登录。

#####  OAUTH与SSO区别

-   从信任角度来看，`OAUTH`开放授权的服务端和第三方客户端不属于一个互相信任的应用群，而单点登录的子系统都在一个互相信任的应用群，通常是同一个公司提供的服务。
-   从资源角度来看，`OAUTH`开放授权主要是让用户自行决定在服务端的个人资源是否允许第三方应用访问，而单点登录的资源本身都在子系统这边，主要服务是用于登录，以及管理用户在各个子系统的权限信息。

##### 实现方案

######  共享SESSION

如果系统是使用`SESSION`来记录用户信息的话，那么就可以采用共享`SESSION`的方式进行实现单点登录，使用`SESSION`信息作为单点登录的方式就需要解决两个问题，一是子系统的`SESSION`是相互隔离的问题，二是用户的`SESSIONID`如何在客户端共享的问题。
`SESSION`的一致性的解决方案主要有`SESSION`同步、`SESSION`集中存储的方式，`SESSION`同步比较消耗资源，所以一般还是使用`SESSION`集中存储的方式。
对于`SESSIONID`在客户端共享的问题，`SESSIONID`主要还是存储在`COOKIE`中，所以需要解决的问题是`COOKIE`的跨域问题，对于同一个顶级域名下的二级域名，可以通过在`SET-COOKIE`时设置`domain`属性为顶级域名，即可实现在顶级域名与二级域名三级域名下的`COOKIE`共享，若是需要非子域名下的`COOKIE`共享，可以考虑使用`P3P`隐私参考项目平台`Platform for Privacy Preferences`的`header`的方式跨域`SET-COOKIE`。

###### Ticket

`Ticket`方式也称为`SSO-Token`，其是一个用户身份的标识，这个标识在所有子系统群中是唯一的，并且所有的子系统`SERVER`都可以验证这个`Token`并同时能够拿到这个`Token`所代表的用户信息，同样这种方式也需要解决`COOKIE`的跨域问题，同样一般也是需要使用顶级域名的`domain`属性或者`P3P`的`header`的跨域`SET-COOKIE`。

######  CAS

`CAS`中央认证服务`Central Authentication Service`，将认证服务单独抽出作为一个子系统，所有的登录认证服务都在`CAS`认证中心进行。`CAS`系统像是一个中转中心，可以认证所有用户的身份，同样也可以直接通过在`CAS`系统登录后以登录态跳转到其他各个系统。
假如我们存在三个子系统，`A`系统`A.com`、`B`系统`B.com`、认证服务`SSO.com`，当用户已经注册，登录时的主要流程：

-   用户打开系统`A`，此时用户未登录，系统自动跳转到认证服务系统`SSO.com`并携带参数存储跳转地址`A.com`。
-   用户在`SSO.com`输入账号密码，点击登录验证成功后，中央认证服务器返回一个`Ticket`，并将已经登录的`COOKIE`写入`SSO.com`认证服务的域名下，`SSO.com`认证服务重定向至跳转到认证服务时携带的地址，也就是上一步的`A.com`，并携带中央认证服务端下发的`Ticket`。
-   系统`A`得到`Ticket`并向本系统的服务器传递`Ticket`，服务端验证`Ticket`无误后获取`Ticket`中携带的用户信息，并设置当前`A`系统的此用户为登录态，下发`COOKIE`信息等用户凭据，至此该用户可正常使用`A`系统的服务。
-   此时用户打开`B`系统，由于用户未在`B`系统登录，系统自动跳转到认证服务系统`SSO.com`并携带参数存储跳转地址`B.com`。
-   用户在`SSO.com`已经处于登录状态，此时直接从中央认证服务器获取`Ticket`，然后重定向至跳转到认证服务时携带的地址，也就是上一步的`B.com`，并携带中央认证服务端下发的`Ticket`。
-   系统`B`得到`Ticket`并向本系统的服务器传递`Ticket`，服务端验证`Ticket`无误后获取`Ticket`中携带的用户信息，并设置当前`B`系统的此用户为登录态，下发`COOKIE`信息等用户凭据，至此该用户可正常使用`B`系统的服务。

#### OAuth 2.0

[OAuth2.0原理图解：第三方网站为什么可以使用微信登录](https://juejin.cn/post/7066716559808397343)

-   用户通过客户端（可以是浏览器也可以是手机应用）想要访问 SP 上的资源，但是 SP 告诉用户需要进行认证，将用户重定向至 IdP
-   IdP 向用户询问 SP 是否可以访问用户信息，如果用户同意，IdP 向客户端返回 access code
-   客户端拿 code 向 IdP 换 access token，并拿着 access token 向 SP 请求资源
-   SP 接受到请求之后拿着附带 token 向 IdP 验证用户的身份

用户从 IdP 返回客户端的方式是通过 URL 重定向，这里的 URL 允许自定义schema，所以即使在手机上也能拉起应用；另一方面因为 IdP 向客户端传递的是 code，而不是 XML 信息，所以 code 可以很轻易的附着在重定向 URL 上进行传递

但以上的 SSO 流程体现不出 OAuth 的本意。**OAuth 的本意是一个应用允许另一个应用在用户授权的情况下访问自己的数据,OAuth 的设计本意更倾向于授权而非认证（当然授权用户信息就间接实现了认证）**, 虽然 Google 的 OAuth 2.0 API 同时支持授权和认证。所以你在使用 Facebook 或者 Gmail 账号登陆第三方站点时，会出授权对话框告诉你第三方站点可以访问你的哪些信息，需要征得你的同意：

#### Refresh Token

为什么我们需要 refresh token？

这样的处理是为了职责的分离：refresh token 负责身份认证，access token 负责请求资源。虽然 refresh token 和 access token 都由 IdP 发出，但是 access token 还要和 SP 进行数据交换，如果公用的话这样就会有身份泄露的可能。并且 IdP 和 SP 可能是完全不同的服务提供的。而在第一小节中我们之所以没有这样的顾虑是因为 IdP 和 SP 都是 Google



![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2018/7/3/164608d01d87803a~tplv-t2oaga2asx-zoom-in-crop-mark:1304:0:0:0.awebp)

### 7.分布式SESSION一致性

`SESSION`是服务器为客户端创建的一个会话，存储用户的相关信息，用以标识用户身份等。在单服务器环境下是不需要考虑会话的一致性的问题的，但是在集群环境下就会出现一些问题，假如一个用户在登录请求时负载均衡到了`A`服务器，`A`服务器为其分配了`SESSION`，下次请求数据时被分配到了`B`服务器，此时由于`B`服务器不存在此用户的`SESSION`，此用户会被重定向到登录页面，这种情况是不合理的业务逻辑，所以需要维护`SESSION`的一致性。

#### 解决方案

##### [SESSION]() 同步

多个服务器之间互相同步`SESSION`，即`A`服务器生成一个`SESSION`信息后同步传输到`B`、`C`、`D`等服务器，同样`B`、`C`、`D`服务器生成`SESSION`信息后也需要同步到`A`，这样每个服务器之间都包含全部的`SESSION`

##### 优点

- 大部分应用服务器都提供了`SESSION`复制的功能来实现集群

##### 缺点

- `SESSION`需要网络传输进行同步，其会占用带宽，并且存在一定的延迟
- 一旦某台机器的`SESSION`信息有所变化，必须同步更新所有服务器`SESSION`内容
- 每个服务器都会存储全部的用户信息，性能随着服务器增加急剧下降，而且容易引起广播风暴

#### SESSION 映射

通过将负载均衡服务器进行修改，通过对返回给用户的`SESSION ID`或者用户请求的`IP`地址进行标记，也就是使用第四层传输层中读取网络层的`IP`或者是在第七层中读取`HTTP`协议中某些属性来做`HASH`，保证对于此用户的请求全部落到同一台服务器上

##### 优点

- 实现相对简单
- 只要分配服务器时均匀，则多台服务器是负载均衡的

##### 缺点

- 一旦某台服务器宕机，则会影响落在此服务器请求上的全部用户
- 负载均衡服务器变为了一个有状态的节点，内存消耗会更大，容灾更麻烦

#### 客户端存储

将数据直接存储到客户端比如`Cookie`或请求头中，每次请求客户端自动携带数据信息

##### 优点

- 简单，高效
- 服务端不需要储存标记用户信息

##### 缺点

- 安全性较差，对于敏感信息必须加密
- 每次请求可能携带大量数据，占用外网带宽
- 数据存储在客户端就会存在泄密、篡改、窃取等隐患

#### 后端集中存储

将`SESSION`存储在一台单独的服务器中的数据库中，例如`Mysql`、`Oracle`、`SqlServer`、`Redis`、`Mongodb`等等，各`SERVER`服务器需要用户信息时携带`SESSION ID`对于集中存储服务器进行请求，进而获取用户信息

##### 优点

- 没有安全隐患
- 可以方便的水平拓展
- `SERVER`服务器重启不会造成`SESSION`丢失

##### 缺点

- 每次请求都增加了一次对于存储服务器的网络请求
- 会对集中存储服务器存在大量请求，数据库压力比较大

## 正则表达式

[regex101.com](https://www.bilibili.com/video/BV1yo4y1d7UK/?spm_id_from=333.788.recommend_more_video.0)

### 限定符

`？` 指前面的一个字符出现0次或者1次 可有可无

 `*`   指前面的一个字符出现0次或者多次    

 `+`   指前面的一个字符出现至少1次   就相当于正区间

`{}`   指前面的一个字符出现次数范围  {2,6}  2-6次  {2,} 至少2次

  `{ab}+`    这样匹配了ab多次

### 或运算符

`a  (cat|dog)`  可以匹配 a cat 或者 a dog

### 字符类

`[abc]+`     匹配有`abc`的字符 

`[a-z]`   `[A-Z]`        `[0-9]`     `[a-zA-Z]`    `[a-zA-Z0-9]`

`^`    `[^0-9]+`     不含数字的字符

### 元字符

`\d`    数字字符

`\D`    非数字字符

`\w`   单词字符(英文，数字以及下划线)

`\W`   非单词字符

`\s`    空白字符(包含Tab，制表符以及换行符)

`\S`   非空白字符

`.`      任意字符 不包含换行符

`^`    只匹配行首

`$`    只匹配行尾



### 贪婪与懒惰匹配

`*`  `+`  `{}`   会尽可能多的去匹配字符

比如匹配html标签  `<.+?>`   `?`将贪婪模式设置为懒惰模式



`\b 和 \B`

\b 是单词边界，具体就是 \w 与 \W 之间的位置，也包括 \w 与 ^ 之间的位置，和 \w 与 $ 之间的位置

`(?=p) 和 (?!p)`



###  实例

#### RGB颜色值匹配

  `#[a-fA-F0-9]{6}\b`      \b设置单词字符的边界

16进制颜色

`/#([0-9a-fA-F]{6}|[0-9a-fA-F]{3})/g`

### 时间匹配

#### 时刻匹配

`/^[01][0-9]|[2][0-3]):[0-5][0-9]$/`

**如果省略0**

`/^(0?[0-9]|1[0-9]|[2][0-3]):(0?[0-9]|[1-5][0-9])$/`

#### 日期匹配

`/^[0-9]{4}-(0[1-9]|1[0-2])-(0[1-9]|[12][0-9]|3[01]$/g`

### IPv4匹配

`\d+.\d+.\d+.\d+`     

### js用法

regex.test(str)

str.match(regex)

str.exec(regex)

## 面向对象理论

### 1.为什么选择面向对象编程

①易扩展：由于面向对象编程封装、继承、多态的三大特性，使得设计出来的系统高内聚、低耦合，整个系统灵活、易扩展，而且成本较低。
②程序的鲁棒性好：开发过程中可以重用已有的并在相关领域经过长期测试的代码，这对系统的鲁棒性有良好的促进作用。
③效率高：在软件开发时，根据需要将现实世界的事物抽象成类，类的属性表示事物的特有性质、类的方法表示事物的行为。一个类对象表示一个事物的实例。这种方式更贴近人类的日常思维，能够提高程序的开发效率和质量。

### 2.面向对象的特性

#### （1）封装

即隐藏对象的属性和实现细节，仅对外公开接口。
封装的优点如下：
①防止使用者有意无意地篡改重要的系统属性
②提高了各个子系统之间的松耦合性，提高系统的独立性。当一个系统内部发生改动时，只要对外提供的接口不变，就不会影响到其他的系统。
③提高软件的可重用性。每个系统都是一个相对独立的整体，可以在多种环境中得到重用。
④降低了建构大型系统的风险。即使整个系统不成功，个别独立的子系统依然是有价值的。

#### （2）继承

子类可以继承父类的属性和方法。同时，子类中还可以扩展出新的属性和方法。
继承的优点如下：
提高了系统的可重用性和可扩展性。

#### （3）多态

多态指当系统A访问系统B的服务时，系统B可以通过多种实现方式来提供服务。主要包括：重载（overload）和重写（override）两种方式。
重载必须满足以下条件：
①方法名相同
②参数的个数、顺序、类型至少有1项不同
③返回类型可以不同（两个方法不能仅有返回类型不同！！！）
④修饰符可以不同

重写发生在父类与子类之间，必须满足以下条件:
①方法名、参数、返回值一定要相同
②子类方法不能缩小父类方法的访问权限
③子类方法不能比父类抛出更多的异常
④父类的非静态方法不能被子类覆盖为静态方法
⑤父类的静态方法不能被子类覆盖为非静态的方法
⑥子类可以定义与父类静态方法同名的静态方法
⑦子类不能覆盖父类的私有方法

注：重载与重写的区别：
①重载要求两个函数的函数名相同，但参数的个数、种类或者类型不同、返回类型不同（可以仅仅参数不同，不能仅仅只有返回类型不同）；重写要求两个函数的返回值类型、函数名、参数完全相同

### 3、面向对象编程的原则

（1）单一职责原则：设计一个类时要考虑好类的属性和方法。不要有多余的方法，否则会出现权责不明，对程序的开发和维护都十分不利。
（2）开放封闭原则：对类的扩展是开放的（可以创建子类继承父类或者抽象方法、也可以创建类来实现接口，这样可以提高代码的可重用性），对类的更改是封闭的（尽量减少对已有类的更改，否则可能会引发一系列错误）。
（3）依赖倒置原则：在MVC设计模式中，Control层依赖于Service层，Service层依赖于Dao层，这叫依赖实现编程，这种方式有一个缺点就是各层之间的耦合性太强； 现将MVC各层抽象，在Control层和Service层之间建立关系，在Service和Dao层之间建立关系，这叫依赖抽象编程，这种方式降低了各层之间的耦合性，提高了独立性。
从“依赖实现编程”到“依赖抽象编程”的转变，即是依赖倒置原则
（4）里氏代换原则：子类必须能够替换掉他们的父类
（5）迪米特原则：两个没有直接联系的类之间要发生相互作用，可以通过第三个类来实现。
（6）组合原则：尽量少用类之间的继承，尽量使用组合。过多的继承会是整个系统变得复杂。而组合是一种用多个简单子系统组装出复杂系统的有效手段。

## 操作系统原理

### 1.进程和线程

进程（Process）是系统进行资源分配和调度的基本单位。

进程也是抢占处理机的调度单位，它拥有一个完整的虚拟地址空间。当进程发生调度时，不同的进程拥有不同的虚拟地址空间，而同一进程内的不同线程共享同一地址空间。

与进程相对应，线程与资源分配无关，它属于某一个进程，并与进程内的其他线程一起共享进程的资源。

线程只由相关堆栈（系统栈或用户栈）寄存器和线程控制表TCB组成。寄存器可被用来存储线程内的局部变量，但不能存储其他线程的相关变量。

调度与操作系统的线程的实现有关，如果是管态线程与目态线程是一一对应，则调度的最小单位可以是线程，但我觉得这也就是理论上，一般的商用操作系统可能操作系统调度的单位也是进程。

[![img](https://iknow-pic.cdn.bcebos.com/ac4bd11373f08202d6a9f18045fbfbedab641b51?x-bce-process=image%2Fresize%2Cm_lfit%2Cw_600%2Ch_800%2Climit_1%2Fquality%2Cq_85%2Fformat%2Cf_auto)](https://iknow-pic.cdn.bcebos.com/ac4bd11373f08202d6a9f18045fbfbedab641b51)



进程一般有三个状态：就绪状态、执行状态和等待状态【或称阻塞状态】；进程只能由父进程建立，系统中所有的进程形成一种进程树的层次体系；挂起命令可由进程自己和其他进程发出，但是解除挂起命令只能由其他进程发出。

进程控制块（PCB）：PCB不但可以记录进程的属性信息，以便操作系统对进程进行控制和管理，而且PCB标志着进程的存在，操作系统根据系统中是否有该进程的进程控制块PCB而知道该进程存在与否。

系统建立进程的同时就建立该进程的PCB，在撤销一个进程时，也就撤销其PCB，故进程的PCB对进程来说是它存在的具体的物理标志和体现。一般PCB包括以下三类信息：进程标识信息；处理器状态信息；进程控制信息。

### 2.进程间通信

前端领域已经不是单纯写在浏览器里跑的页面就可以了，还要会 electron、nodejs 等，而这俩技术都需要掌握进程通信。

nodejs 是 js 的一个运行时，和浏览器不同，它扩展了很多封装操作系统能力的 api，其中就包括进程、线程相关 api，而学习进程 api 就要学习进程之间的通信机制。

electron 是基于 chromium 和 nodejs 的桌面端开发方案，它的架构是一个主进程，多个渲染进程，这两种进程之间也需要通信，要学习 electron 的进程通信机制。

这篇文章我们就来深入了解一下进程通信。

本文会讲解以下知识点：

-   进程是什么
-   本地进程通信的四种方式
-   ipc、lpc、rpc 都是什么
-   electron 如何做进程通信
-   nodejs 的 child_process 和 cluster 如何做进程通信
-   进程通信的本质

#### 进程

我们写完的代码要在操作系统之上跑，操作系统为了更好的利用硬件资源，支持了多个程序的并发和硬件资源的分配，分配的单位就是进程，这个进程就是程序的执行过程。比如记录程序执行到哪一步了，申请了哪些硬件资源、占用了什么端口等。

进程包括要执行的代码、代码操作的数据，以及进程控制块 PCB（Processing Control Block），因为程序就是代码在数据集上的执行过程，而执行过程的状态和申请的资源需要记录在一个数据结构（PCB）里。所以进程由代码、数据、PCB 组成。

![img](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d1d553640791413c819d93ad442b2032~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

pcb 中记录着 pid、执行到的代码地址、进程的状态（阻塞、运行、就绪等）以及用于通信的信号量、管道、消息队列等数据结构。

![img](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7c7eea0b10bb43a8aa821fe2de880cbe~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

进程从创建到代码不断的执行，到申请硬件资源（内存、硬盘文件、网络等），中间还可能会阻塞，最终执行完会销毁进程。这是一个进程的生命周期。

进程对申请来的资源是独占式的，每个进程都只能访问自己的资源，那进程之间怎么通信呢？

#### 进程通信

不同进程之间因为可用的内存不同，所以要通过一个中间介质通信。

##### 信号量

如果是简单的标记，通过一个数字来表示，放在 PCB 的一个属性里，这叫做`信号量`，比如锁的实现就可以通过信号量。

这种信号量的思想我们写前端代码也经常用，比如实现节流的时候，也要加一个标记变量。

##### 管道

但是信号量不能传递具体的数据啊，传递具体数据还得用别的方式。比如我们可以通过读写文件的方式来通信，这就是`管道`，如果是在内存中的文件，叫做匿名管道，没有文件名，如果是真实的硬盘的文件，是有文件名的，叫做命名管道。

文件需要先打开，然后再读和写，之后再关闭，这也是管道的特点。管道是基于文件的思想封装的，之所以叫管道，是因为只能一个进程读、一个进程写，是单向的（半双工）。而且还需要目标进程同步的消费数据，不然就会阻塞住。

这种管道的方式实现起来很简单，就是一个文件读写，但是只能用在两个进程之间通信，只能同步的通信。其实管道的同步通信也挺常见的，就是 stream 的 pipe 方法。

##### 消息队列

管道实现简单，但是同步的通信比较受限制，那如果想做成异步通信呢？加个队列做缓冲（buffer）不就行了，这就是`消息队列`。

消息队列也是两个进程之间的通信，但是不是基于文件那一套思路，虽然也是单向的，但是有了一定的异步性，可以放很多消息，之后一次性消费。

##### 共享内存

管道、消息队列都是两个进程之间的，如果多个进程之间呢？

我们可以通过申请一段多进程都可以操作的内存，叫做`共享内存`，用这种方式来通信。各进程都可以向该内存读写数据，效率比较高。

共享内存虽然效率高、也能用于多个进程的通信，但也不全是好处，因为多个进程都可以读写，那么就很容易乱，要自己控制顺序，比如通过进程的信号量（标记变量）来控制。

共享内存适用于多个进程之间的通信，不需要通过中间介质，所以效率更高，但是使用起来也更复杂。

上面说的这些几乎就是本地进程通信的全部方式了，为什么要加个本地呢？

#### ipc、rpc、lpc

进程通信就是 ipc（Inter-Process Communication），两个进程可能是一台计算机的，也可能网络上的不同计算机的进程，所以进程通信方式分为两种：

本地过程调用 LPC（local procedure call）、远程过程调用 RPC（remote procedure call）。

本地过程调用就是我们上面说的信号量、管道、消息队列、共享内存的通信方式，但是如果是网络上的，那就要通过网络协议来通信了，这个其实我们用的比较多，比如 http、websocket。

所以，当有人提到 ipc 时就是在说进程通信，可以分为本地的和远程的两种来讨论。

远程的都是基于网络协议封装的，而本地的都是基于信号量、管道、消息队列、共享内存封装出来的，比如我们接下来要探讨的 electron 和 nodejs。

#### electron 进程通信

electron 会先启动主进程，然后通过 BrowserWindow 创建渲染进程，加载 html 页面实现渲染。这两个进程之间的通信是通过 electron 提供的 ipc 的 api。

##### ipcMain、ipcRenderer

主进程里面通过 ipcMain 的 on 方法监听事件

```javascript
import { ipcMain } from 'electron';

ipcMain.on('异步事件', (event, arg) => {
  event.sender.send('异步事件返回', 'yyy');
})
```

渲染进程里面通过 ipcRenderer 的 on 方法监听事件，通过 send 发送消息

```javascript
import { ipcRenderer } from 'electron';

ipcRender.on('异步事件返回', function (event, arg) {
  const message = `异步消息: ${arg}`
})

ipcRenderer.send('异步事件', 'xxx')
```

api 使用比较简单，这是经过 c++ 层的封装，然后暴露给 js 的事件形式的 api。

我们可以想一下它是基于哪种机制实现的呢？

很明显有一定的异步性，而且是父子进程之间的通信，所以是消息队列的方式实现的。

##### remote

除了事件形式的 api 外，electron 还提供了远程方法调用 rmi （remote method invoke）形式的 api。

其实就是对消息的进一步封装，也就是根据传递的消息，调用不同的方法，形式上就像调用本进程的方法一样，但其实是发消息到另一个进程来做的，和 ipcMain、ipcRenderer 的形式本质上一样。

比如在渲染进程里面，通过 remote 来直接调用主进程才有的 BrowserWindow 的 api。

```javascript
const { BrowserWindow } = require('electron').remote;

let win = new BrowserWindow({ width: 800, height: 600 });
win.loadURL('https://github.com');
```

小结一下，electron 的父子进程通信方式是基于消息队列封装的，封装形式有两种，一种是事件的方式，通过 ipcMain、ipcRenderer 的 api 使用，另一种则是进一步封装成了不同方法的调用（rmi），底层也是基于消息，执行远程方法但是看上去像执行本地方法一样。

#### nodejs

nodejs 提供了创建进程的 api，有两个模块： child_process 和 cluster。很明显，一个是用于父子进程的创建和通信，一个是用于多个进程。

##### child_process

child_process 提供了 spawn、exec、execFile、fork 的 api，分别用于不同的进程的创建：

##### spawn、exec

如果想通过 shell 执行命令，那就用 spawn 或者 exec。因为一般执行命令是需要返回值的，这俩 api 在返回值的方式上有所不同。

spawn 返回的是 stream，通过 data 事件来取，exec 进一步分装成了 buffer，使用起来简单一些，但是可能会超过 maxBuffer。

```javascript
const { spawn } = require('child_process'); 

var app = spawn('node','main.js' {env:{}});

app.stderr.on('data',function(data) {
  console.log('Error:',data);
});

app.stdout.on('data',function(data) {
  console.log(data);
});
```

其实 exec 是基于 spwan 封装出来的，简单场景可以用，有的时候要设置下 maxBuffer。

```javascript
const { exec } = require('child_process'); 

exec('find . -type f', { maxBuffer: 1024*1024 }(err, stdout, stderr) => { 
    if (err) { 
        console.error(`exec error: ${err}`); return; 
    }   
    console.log(stdout); 
});
```

##### execFile

除了执行命令外，如果要执行可执行文件就用 execFile 的 api：

```javascript
const { execFile } = require('child_process'); 

const child = execFile('node', ['--version'], (error, stdout, stderr) => { 
    if (error) { throw error; } 
    console.log(stdout); 
});
```

##### fork

还有如果是想执行 js ，那就用 fork：

```javascript
const { fork } = require('child_process');	

const xxxProcess = fork('./xxx.js');	
xxxProcess.send('111111');	
xxxProcess.on('message', sum => {	
    res.end('22222');	
});
```

#### 小结5

简单小结一下 child_process 的 4 个 api：

如果想执行 shell 命令，用 spawn 和 exec，spawn 返回一个 stream，而 exec 进一步封装成了 buffer。除了 exec 有的时候需要设置下 maxBuffer，其他没区别。

如果想执行可执行文件，用 execFile。

如果想执行 js 文件，用 fork。

#### child_process 的进程通信

说完了 api 我们来说下 child_process 创建的子进程怎么和父进程通信，也就是怎么做 ipc。

##### pipe

首先，支持了 pipe，很明显是通过管道的机制封装出来的，能同步的传输流的数据。

```javascript
const { spawn } = require('child_process'); 

const find = spawn('cat', ['./aaa.js']);
const wc = spawn('wc', ['-l']);  find.stdout.pipe(wc.stdin);
```

比如上面通过管道把一个进程的输出流传输到了另一个进程的输入流，和下面的 shell 命令效果一样：

```shell
cat ./aaa.js | wc -l
```

##### message

spawn 支持 stdio 参数，可以设置和父进程的 stdin、stdout、stderr 的关系，比如指定 pipe 或者 null。还有第四个参数，可以设置 ipc，这时候就是通过事件的方式传递消息了，很明显，是基于消息队列实现的。

```javascript
const { spawn } = require('child_process');

const child = spawn('node', ['./child.js'], {
    stdio: ['pipe', 'pipe', 'pipe', 'ipc'] 
}); 
child.on('message', (m) => { 
    console.log(m); 
}); 
child.send('xxxx');
```

而 fork 的 api 创建的子进程自带了 ipc 的传递消息机制，可以直接用。

```javascript
const { fork } = require('child_process');	

const xxxProcess = fork('./xxx.js');	
xxxProcess.send('111111');	
xxxProcess.on('message', sum => {	
    res.end('22222');	
});
```

##### cluster

cluster 不再是父子进程了，而是更多进程，也提供了 fork 的 api。

比如 http server 会根据 cpu 数启动多个进程来处理请求。

```javascript
import cluster from 'cluster';
import http from 'http';
import { cpus } from 'os';
import process from 'process';

const numCPUs = cpus().length;

if (cluster.isPrimary) {
  for (let i = 0; i < numCPUs; i++) {
    cluster.fork();
  }
} else {
  const server = http.createServer((req, res) => {
    res.writeHead(200);
    res.end('hello world\n');
  })
  
  server.listen(8000);
  
  process.on('message', (msg) => {
    if (msg === 'shutdown') {
       server.close();
    }
  });
}
```

它同样支持了事件形式的 api，用于多个进程之间的消息传递，因为多个进程其实也只是多个父子进程的通信，子进程之间不能直接通信，所以还是基于消息队列实现的。

#### 共享内存

子进程之间通信还得通过父进程中转一次，要多次读写消息队列，效率太低了，就不能直接共享内存么？

现在 nodejs 还是不支持的，可以通过第三方的包 shm-typed-array 来实现，感兴趣可以看一下。

[www.npmjs.com/package/shm…](https://link.juejin.cn/?target=https%3A%2F%2Fwww.npmjs.com%2Fpackage%2Fshm-typed-array)

#### 总结

进程包括代码、数据和 PCB，是程序的一次执行的过程，PCB 记录着各种执行过程中的信息，比如分配的资源、执行到的地址、用于通信的数据结构等。

进程之间需要通信，可以通过信号量、管道、消息队列、共享内存的方式。

-   信号量就是一个简单的数字的标记，不能传递具体数据。
-   管道是基于文件的思想，一个进程写另一个进程读，是同步的，适用于两个进程。
-   消息队列有一定的 buffer，可以异步处理消息，适用于两个进程。
-   共享内存是多个进程直接操作同一段内存，适用于多个进程，但是需要控制访问顺序。

这四种是本地进程的通信方式，而网络进程则基于网络协议的方式也可以做进程通信。

进程通信叫做 ipc，本地的叫做 lpc，远程的叫 rpc。

其中，如果把消息再封装一层成具体的方法调用，叫做 rmi，效果就像在本进程执行执行另一个进程的方法一样。

electron 和 nodejs 都是基于上面的操作系统机制的封装：

-   elctron 支持 ipcMain 和 ipcRenderer 的消息传递的方式，还支持了 remote 的 rmi 的方式。
-   nodejs 有 child_process 和 cluster 两个模块和进程有关，child_process 是父子进程之间，cluster 是多个进程：
    -   child_process 提供了用于执行 shell 命令的 spawn、exec，用于执行可执行文件的 execFile，用于执行 js 的 fork。提供了 pipe 和 message 两种 ipc 方式。
    -   cluster 也提供了 fork，提供了 message 的方式的通信。

当然，不管封装形式是什么，都离不开操作系统提供的信号量、管道、消息队列、共享内存这四种机制。

ipc 是开发中频繁遇到的需求，希望这篇文章能够帮大家梳理清楚从操作系统层到不同语言和运行时的封装层次的脉络。

### 3.死锁产生原因和解决

死锁：两个或多个并发进程中，如果每个进程持有某种资源而又都等待着别的进程释放它或它们现在保持着的资源，否则就不能向前推进。此时称这一组进程为死锁

引起死锁的原因和必要条件
引起死锁的原因：
①系统资源不足
②进程推进顺序非法

产生死锁的必要条件
①互斥条件——涉及的资源是非共享的，即为临界资源
②不剥夺条件——进程所获得的资源在未使用完毕之前，不能被其它进程强行夺走
③部分分配——进程每次申请它所需要的一部分资源。在等待新资源的同时，进程继续占用已分配到的资源
④环路条件——存在一种进程的循环链，链中的每一个进程已获得的资源同时被链中下一个进程所请求

为了使系统不发生死锁，必须要破坏产生死锁的四个必要条件之一

①采用资源静态分配方法预防死锁
②采用资源动态分配、有控分配方法来避免死锁
③当死锁发生时检测出死锁，并设法修复
④忽略死锁，一旦发生死锁便重启系统（这种方法被绝大多数操作系统所采用）



死锁的动态避免就是采用资源动态分配的方式
①有序资源分配方法：系统中所有资源都给一个唯一的编号，所有分配请求必须以上升的次序进行，当遵守上升次序的规则时，若资源可用，则予以分配；否则，请求者等待。
②银行家算法：申请者事先说明对各类资源的最大需求量。在进程活动期间动态申请某类资源时，由系统审查现有该类资源的数目是否能满足当前进程的最大需求量，如能满足就予以分配，否则拒绝。



### 4.操作系统资源分配

资源管理目的：为用户提供一种简单而有效地使用资源的方法
任务：1、资源数据结构的描述
2、确定资源的分配原则和调度原则
3、执行资源分配
4、存取控制和安全保护

操作系统对资源区分两种不同的概念
①物理资源——系统中那些物理、可实际使用的资源
②虚拟资源——逻辑资源。是经过操作系统改造的、用户看到的，使用方便的虚资源

目的：①方便用户使用 ②资源可动态分配，提高资源利用率

资源分配机制
资源描述器：描述各类资源的最小分配单位的数据结构
资源信息块：描述某类资源的请求者、可利用的资源以及该类资源分配程序的地址

资源分配策略：在众多个请求者中选一个满足条件的请求者原则

资源分配策略具体如何体现？
体现在资源请求队列的排序原则上
（1）先请求先服务策略（FIFO）
①排序原则——按请求的先后次序排序：每一个新产生的请求均排在资源请求队列的队尾。
②资源可用时的处理：资源可用时，取资源请求队列队首元素，将该资源分配给请求者。

（2）优先调度策略
①排序原则——按请求的优先级高低排序
对每一个进程制定一个优先级
按优先级的高低排序——每一个新产生的请求按对应进程的优先级高低插入到队列的相应位置。
（3）针对设备特性的调度策略
调度目标：当有大量的I/O请求时，降低完成这些I/O服务的总时间
移臂调度：最短寻道时间优先算法（SSTF）、扫描算法（SCAN）
旋转调度

如何确定移动臂磁盘组中磁盘块的物理位置

## 计算机组成原理

### 1.怎么存储数

1.  在JS中能否表示的数字的绝对值范围是5e-324 ~ 1.7976931348623157e+308，这一点可以通过`Number.MAX_VALUE`和`Number.MIN_VALUE`来得到证实
2.  在JS中能够表示的最大安全整数的范围是：-9007199254740991 ~ 9007199254740991，这一点可以通过`Number.MIN_SAFE_INTEGER`和`Number.MAX_SAFE_INTEGER`来求证

存在的问题

1.  在四则运算中存在精度丢失的问题，比如： `01 + 0.2 //0.30000000000000004`
2.  超过最大安全整数的运算是不安全的，比如：`9007199254740991 + 2 // 9007199254740992`

1.  把这个浮点数转成对应的二进制数，并用科学计数法表示
2.  把这个数值通过[IEEE 754](https://link.juejin.cn/?target=https%3A%2F%2Fzh.wikipedia.org%2Fwiki%2FIEEE_754)标准表示成真正会在计算机中存储的值

我们知道，JS中的Number类型使用的是双精度浮点型，也就是其他语言中的double类型。而双精度浮点数使用64 bit来进行存储，结构图如下：

![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2018/1/30/16144bd12f9b3376~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.awebp)

也就是说一个Number类型的数字在内存中会被表示成：`s x m x 2^e`这样的格式。

在[ES规范](https://link.juejin.cn/?target=http%3A%2F%2Fes5.github.io%2F%23x8.5)中规定e的范围在-1074 ~ 971，而m最大能表示的最大数是52个1，最小能表示的是1，这里需要注意：



### 2.什么是补码

#### 原码

一个 `8` 位的存储单元可以存储 `00000000` ～ `11111111` 共 `256` 种数字，真值（即真实数值，用10进制表示）从 `0` ～ `255`。为了引入负数，所以设计出了原码：用存储单元的第一位表示符号，`1` 位负，`0` 为正，后面的位表示真值的绝对值。同样以 `8` 位存储单元位例，在用原码的方式下我们依然可以表示 `256` 个数，从 `11111111` ～ `01111111`，真值范围为 `-127` ～ `127`，可能你发现真值一共只有`255` 个，因为 `10000000` 和 `00000000` 表示的都是 `0`，即`-0`和`+0`。

#### 反码

由于原码中，`+x`和`-x`使用二进制加法（利用加法电路）相加不能等于0，设计了反码，即：如果是正数则保持不变，如果是负数，则除符号位之外按位取反。这样正负2数相加就变成了 `11111111` ，也就是`-0`，注意这里的结果也是反码。

#### 补码

补码的定义就是正数和 `0` 的补码就是原码，负数的补码是其反码 `+1`。`-1` 的原码是 `10000001`，反码是 `11111110`，补码在反码的基础上 `+1`，即为 `11111111`。`-0` 取补码后跟 `0` 是一致的，没有了两个 `0` 的问题，没了-0后多出的`10000000`表示`-128`，真值数也变回了256。这也就是我们今天计算机所用的存储和计算的方式

由于高位溢出舍弃，采用补码的方式，无论是从`11111111(-1)`到`00000000(0)`，还是从`01111111(127)`到`100000000(-128)`都是连续的，所有数其实是一个类似钟表上数字的闭合的环。0是0点位置，-128是6点位置。反码、补码，通过数学上的同余定理中的同余式相加，达到了用加法来实现减法的效果。

### 3.指令，汇编语言，机器码有什么区别和联系

#### 指令

1.机器码由0和1组成的二进制序列，其可读性差，但运行速度快，为方便阅读，人们发明了指令。

2.指令就是把机器码中特定的0和1序列简化为对应的指令，一般为英文缩写。如jmp,add,mov等。

3.不同平台对应的同一操作的机器码可能不同。

#### 指令集

1.不同平台所支持的指令有所差异，把每个平台所支持的指令，称为该平台对应的指令集

2常见的指令集

x86指令集，对应的是[x86]()[框架](https://so.csdn.net/so/search?q=框架&spm=1001.2101.3001.7020)的平台

ARM指令集，······

#### 汇编语言

1.在提高指令阅读性的基础上，又发明了汇编语言

2.用助记符代替机器指令的操作码，用地址符号或标号代替指令或操作数的地址

>   ![img](https://img-blog.csdnimg.cn/20210728092844832.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDgzNjM2MQ==,size_16,color_FFFFFF,t_70)



## 编译原理

### 1.C语言是怎么进行编译的

![image-20220627153351871](https://s2.loli.net/2022/06/27/ORu2A5qIt6jpbGc.png)

高级语言→汇编语言→机器语言（二进制）

#### **1.预处理**

  由.c文件到.i文件，这个过程叫预处理。在源文件被编译前，首先要进行预处理的工作，也就是对源代码进行相对应的展开、替换和清理。

  ①在这个过程中，主要处理的事情：

  ②把代码注释部分去掉，不让其参与编译

  ③将所有的#define删除，并且展开所有的宏定义，简单来说就是字符替换

  ④处理所有的条件编译指令，例如#ifdef、#ifndef、#endif，简单来说就是带#的那些

  ⑤把“stdio.h”文件包含进来，即用“stdio.h”中的内容替换在“#include”位置

#### **2.编译**

  由.i文件到.s文件，这个过程叫编译。编译的过程实质上是把高级语言翻译成机器语言的过程。简单来说，源文件被预处理之后，再以字符流的形式进行处理，进行词法和语法的分析，然后通过汇编器将源代码指令转变成汇编指令、生成相应的汇编文件。

#### **3.汇编**

  由.s文件到.o文件，这个过程叫汇编。汇编是指把汇编语言代码翻译成目标机器指令的过程，也就是把汇编码转换成机器所能识别的二进制，通过把经过汇编之后生成的文件称为目标文件。

#### **4.链接**

  由.o文件到可执行文件，这个过程叫链接。经过汇编之后生成的目标文件并不能立即被执行，还需要由链接器将代码在执行过程中用到的其他目标代码及库文件进行链接，最终生成一个可执行程序。

  假如.c文件中有用到printf函数，那么就需要找到包含该函数的标准库文件，对它进行链接。

### 2.词法分析和文法分析

https://zhuanlan.zhihu.com/p/31096468

### 3.js预编译原理

**首先JavaScript这个预编译和传统的编译是不一样的（可以把js预编译理解为特殊的编译过程）**

-   我们应该已经知道:JavaScript是解释型语言。(解释型语言，就是编译一行，执行一行)
-   传统的编译会经历很多步骤，分词、解析、代码生成什么的
-   下面就给大家分享一下我所理解的JS预编译

**JavaScript运行三部曲** 脚本执行js,引擎都做了什么呢？

1.  **语法分析**

>   先全部扫一遍 看有没有语法错误.

1.  **预编译(执行前一刻)**

>   变量 声明提升 函数声明整体提升

1.  解释执行

>   解释一行执行一行

**函数中:预编译执行四部曲**

1.  创建AO对象 (Activation Object (执行期上下文))
2.  找形参和变量声明,将变量和形参名作为AO属性名,值为undefined
3.  将实参值和形参统一
4.  在函数体里面找函数声明,值赋予函数体

**全局中:预编译三部曲**

1.  创建GO对象(Global Object window就是全局)
2.  找变量声明,将变量声明作为GO对象的属性名，值赋予undifined
3.  找全局里的函数声明，将函数名作为GO对象的属性名，值赋予函数体

**实例分析**

```js
<script>
    var a = 1;
    console.log(a);
    function test(a) {
        console.log(a);
        var a = 123;
        console.log(a);
        function a() {}
        console.log(a);
        var b = function() {}
        console.log(b);
        function d() {}
    }
    var c = function (){
        console.log("I at C function");
    }
    console.log(c);
    test(2);
</script>
```

**分析过程如下：**

1.  页面产生便创建GO全局对象（Global Object）（也就是window对象）；
2.  第一个脚本文件加载；
3.  脚本加载完毕后，分析语法是否合法；
4.  开始预编译

-   查找变量声明，作为GO属性，值赋予undefined；
-   查找函数声明，作为GO属性，值赋予函数体；

**全局预编译结束后，GO中存储的值**

```js
    //抽象描述
GO/window = {
        a: undefined,
        c: undefined，
        test: function(a) {
            console.log(a);
            var a = 123;
            console.log(a);
            function a() {}
            console.log(a);
            var b = function() {}
            console.log(b);
            function d() {}
        }
    }
```

**解释执行代码（直到执行完test(2)语句）**

```js
GO/window = {
        a: 1,
        c: function (){
            console.log("I at C function");
        }
        test: function(a) {
            console.log(a);
            var a = 123;
            console.log(a);
            function a() {}
            console.log(a);
            var b = function() {}
            console.log(b);
            function d() {}
        }
    }
```

**执行函数test()之前，再次发生预编译**

```
根据函数中:预编译执行四部曲可知
```

预编译第一和第二两小步如下：

```js
//抽象描述
    AO = {
        a:undefined,
        b:undefined,
    }
```

预编译之第3步如下：

```js
 //抽象描述
    AO = {
        a:123,
        b:undefined,
    }
```

预编译之第4步如下：

```js
//抽象描述
    AO = {
        a:function a() {},
        b:undefined
        d:function d() {}
    }
```

执行test()函数时如下过程变化：

```js
 //抽象描述
    AO = {
        a:function a() {},
        b:undefined
        d:function d() {}
    }
    --->
    AO = {
        a:123,
        b:undefined
        d:function d() {}
    }
    --->
    AO = {
        a:123,
        b:function() {}
        d:function d() {}
    }
```

执行结果：

![QQ截图20210423225850.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3cacb058399f4f6d8779bf222f55ba26~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

**注意：** 预编译阶段发生变量声明和函数声明，没有赋值行为，匿名函数不参与预编译 ；只有在解释执行阶段才会进行变量初始化 。

**预编译小节**

1.  预编译两个小规则
    1.  函数声明整体提升—(无论函数调用和声明的位置是前是后，系统总会把函数声明移到调用前面）
    2.  变量 声明提升—(无论变量调用和声明的位置是前是后，系统总会把声明移到调用前，注意仅仅只是声明，所以值是undefined）
2.  预编译前奏
    1.  即任何变量，如果未经声明就赋值，则此变量就位全局变量所有。(全局域就是Window)
    2.  一切声明的全局变量，全是window的属性，如：var a=12;等同于Window.a = 12;
    3.  函数预编译发生在函数执行前一刻。

### 4.v8引擎是怎么编译js的

一个接受Javascript代码，编译代码然后执行的C++程序，编译后的代码可以在多种操作系统多种处理器上运行

#### 主要工作

-   编译js代码
-   处理调用栈
-   内存分配
-   垃圾的回收

#### 重要组件

大部分js引擎在编译和执行js代码，都会用到三个重要的组件

-   解析器：负责将JS源代码解析成抽象语法树（AST）
-   解释器： 负责将AST解释成字节码bytecode，同时解释器也有直接解释执行bytecode的能力
-   编译器：负责编译出运行更加高效的机器代码

#### 优化后的V8

语法树的解析还是基本保持一致的，但在获得抽象语法树之后，v8引擎加入了解释器Ignition，语法树通过解释器Ignition生成了bytecode字节码，此时AST就被清除掉了，释放内存空间，生成bytecode直接被解释器执行，同时生成的bytecode将作为基准执行模型，字节码更加简洁。生成的bytecode大小相当于等效的基准机器代码的25到50%左右。
在代码不断运行过程中，解释器收集到了很多可以用来优化代码的信息，比如变量的类型、那些函数执行的频率较高，这些信息被发送给编译器TruboFan，编译起TruboFan会根据这些信息来编译出经过优化的机器代码。
优化的机器码也有可能被反向编译为字节码，这个过程叫deoptimization ![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/92312258778c42b5bc444d44280653c0~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp?)

**优化策略**

1.  函数只声明未被调用，不会被解析成AST
2.  函数只被调用一次，bytecode直接被解释执行
3.  如果函数被调用多次，可能会被标记为热点函数，可能会被编译成机器代码

**优点**

1.  由于不需要直接编译成机器码，而是使用了中间层的字节码，字节码生成速度远远大于机器吗，所以网页初始化解析执行js的时间缩短了
2.  在生成的优化机器代码时，不需要再从源码开始编译，而是直接使用字节码编译。而且需要deoptimization时，只需要回归到中间层字节码解释执行就可以了

![dcb503d286cc4fada008402da73769b1](https://s2.loli.net/2022/06/27/5YiUpkLnNOstFf2.png)

-   首先 V8 引擎会扫描所有的源代码，进行词法分析，生成 Tokens;
-   Parser 解析器根据 Tokens 生成 AST；
-   Ignition 解释器将 AST 转换为字节码，并解释执行；
-   TurboFan 编译器负责将热点函数优化编译为机器指令执行；

#### 词法分析

V8 引擎首先会扫描所有的源代码，进行词法分析（词法分析是通过 Scanner 模块来完成的，本文不进行详细介绍）。

**什么是词法分析？**

词法分析（Tokenizing/Lexing）其作用是将一行行的源码拆解成一个个 token。所谓**词法单元 token**，指的是语法上不可能再分的、最小的单个字符或字符串。

ECMAScript 中明确定义了 Token 包含的内容。

![img](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aae831255cc64f559f36fbe072873b36~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

我们来看下`var a = 2;` 这句代码经过词法分析后会被分解出哪些 tokens ? ![Image  11](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fc41c8f36d1c40ea81cfa7ac445ecb90~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

从上图中可以看到，这句代码最终被分解出了五个词法单元：

-   `var` 关键字
-   `a` 标识符
-   `=` 运算符(符号)
-   `2` 数值
-   `；`分号(符号)

>   Tokens 在线查看网站：[esprima.org/demo/parse.…](https://link.juejin.cn/?target=https%3A%2F%2Fesprima.org%2Fdemo%2Fparse.html%23)

#### 语法分析

#### Parser

Parser 是 V8 的解析器，负责根据生成的 Tokens 进行语法分析。Parser 的主要工作包括：

-   **分析语法错误**：遇到错误的语法会抛出异常；
-   **输出 AST**：将词法分析输出的词法单元流（数组）转换为一个由元素逐级嵌套所组成的代表了程序语法结构的树——抽象语法树（Abstract Syntax Tree, AST）；
-   **确定词法作用域**；

-   **生成执行上下文**；

**什么是抽象语法树（Abstract Syntax Tree, AST）？**

还是上面的例子，我们来看下 `var a = 2;` 经过语法分析后生成的 AST 是什么样子的：

![Image  12](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1072cb497ad64184b12beb8eb7b8b863~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp) ![Image  13](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7ddc93567e0e4a09a7450c8cb7acd18b~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

可以看到这段程序的类型是 VariableDeclaration，也就是说这段代码是用来声明变量的。

>   AST 在线查看网站：[astexplorer.net/](https://link.juejin.cn/?target=https%3A%2F%2Fastexplorer.net%2F)

AST 的结构和代码的结构非常相似，其实你也可以把 AST 看成代码的结构化表示，编译器或者解释器后续的工作都需要依赖于 AST，而不是源代码。

>   AST 是非常重要的一种数据结构，在很多项目中有着广泛的应用。其中最著名的一个项目就是 Babel。Babel 是一个被广泛使用的代码转码器，可以将 ES6 代码转为 ES5 代码，这意味着你可以现在就用 ES6 编写程序，而不用担心现有环境是否支持 ES6。Babel 的工作原理就是先将 ES6 源码转换为 AST，然后再将 ES6 语法的 AST 转换为 ES5 语法的 AST，最后利用 ES5 的 AST 生成 JavaScript 源代码。 除了 Babel 外，还有 ESLint 也使用 AST。ESLint 是一个用来检查 JavaScript 编写规范的插件，其检测流程也是需要将源码转换为 AST，然后再利用 AST 来检查代码规范化的问题。

#### Pre-Parser

**什么是预解析 Pre-Parser？**

我们先来看看下面这段代码：

```
function foo () {
    console.log('function foo')
}

function bar () {
    console.log('function bar')
}

foo()
```

上面这段代码中，如果使用 Parser 解析后，会生成 foo 函数 和 bar 函数的 AST。然而 bar 函数并没有被调用，所以生成 bar 函数的 AST 实际上是没有任何意义且浪费时间的。那么有没有办法解决呢？此时就用到了 Pre-Parser 技术。

在 V8 中有两个解析器用于解析 JavaScript 代码，分别是 Parser 和 Pre-Parser 。

-   Parser 解析器又称为 full parser（全量解析） 或者 eager parser（饥饿解析）。它会解析所有**立即执行**的代码，包括语法检查，生成 AST，以及确定词法作用域。
-   Pre-Parser 又称为惰性解析，它只解析**未被立即执行**的代码（如函数），不生成 AST ，只确定作用域，以此来提高性能。当预解析后的代码开始执行时，才进行 Parser 解析。

我们还是以示例来说明：

```
function foo() {
    console.log('a');
    function inline() {
        console.log('b')
    }
}

(function bar() {
    console.log('c')
})()；

foo();
```

1.  当 V8 引擎遇到 foo 函数声明时，发现它未被立即执行，就会采用 Pre-Parser 对其进行解析（inline 函数同）。
2.  当 V8 遇到`(function bar() {console.log(c)})()`时，它会知道这是一个立即执行表达式（IIFE），会立即被执行，所以会使用 Parser 对其解析。
3.  当 foo 函数被调用时，会使用 Parser 对 foo 函数进行解析，此时会对 inline 函数再进行一次预解析，也就是说 inline 函数被预解析了两次。如果嵌套层级较深，那么内层的函数会被预解析多次，所以在写代码时，**尽可能避免嵌套多层函数**，会影响性能。

#### Ignition

Ignition 是 V8 的解释器，它负责的工作包括：

-   将 AST 转换为中间代码（字节码 Bytecode）
-   逐行解释执行字节码：在该阶段，就已经可以开始执行 JavaScript 代码了。

**什么是字节码？**

字节码（Bytecode）是介于 AST 和机器码之间的一种中间码，它比机器码更抽象，也更轻量，需要直译器转译后才能成为机器码。

>   早期版本的 V8 ，并没有生成中间字节码的过程，而是直接将 AST 转换为机器码，由于执行机器码的效率是非常高效的，所以这种方式在发布后的一段时间内运行效果是非常好的。但是随着 Chrome 在手机上的广泛普及，特别是运行在 512M 内存的手机上，内存占用问题也暴露出来了，因为 V8 需要消耗大量的内存来存放转换后的机器码。为了解决内存占用问题，V8 团队大幅重构了引擎架构，引入字节码，并且抛弃了之前的编译器，最终花了将进四年的时间，实现了现在的这套架构。

![Image  14](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fee675cd385d47e8bc619fcc8bcaf9f0~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)![img](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0e1b7a71efd7444bbf8d5f5e01f996da~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

从图中可以看出，机器码所占用的空间远远超过了字节码，所以使用字节码可以减少系统内存的占用。

#### TurboFan

TurboFan 是 V8 的优化编译器，负责将字节码和一些分析数据作为输入并生成优化的机器代码。

上面我们说到，当 Ignition 将 JavaScript 代码转换为字节码后，程序就可以执行了，那么 TurboFan 还有什么用呢？

我们再来看下 V8 的工作流程图： ![Image  15](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8ab6dc853dda4427913ac2882100c783~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

我们主要关注 Ignition 和 TurboFan 的交互： ![Image  16](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8134069a2ada4c349f4743da4c311690~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

当 Ignition 开始执行 JavaScript 代码后，V8 会一直观察 JavaScript 代码的执行情况，并记录执行信息，如每个函数的执行次数、每次调用函数时，传递的参数类型等。

如果一个函数被调用的次数超过了内设的阈值，监视器就会将当前函数标记为热点函数（Hot Function），并将该函数的字节码以及执行的相关信息发送给 TurboFan。TurboFan 会根据执行信息做出一些进一步优化此代码的假设，在假设的基础上将字节码编译为优化的机器代码。如果假设成立，那么当下一次调用该函数时，就会执行优化编译后的机器代码，以提高代码的执行性能。

>   V8 的解释器和编译器的取名也很有意思。解释器 Ignition 是点火器的意思，编译 TurboFan 是涡轮增压的意思，寓意着代码启动时通过点火器慢慢发动，一旦启动，涡轮增压介入，其执行效率随着执行时间越来越高效率，因为热点代码都被编译器 TurboFan 转换了机器码，直接执行机器码就省去了字节码“翻译”为机器码的过程。我们把这种技术称为即时编译（JIT）

![img](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f22603639cae4187b737a175c23c2f68~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp)

那如果假设不成立呢？不知道你们有没有注意到上图中有一条由 optimized code 指向 bytecode 的红色指向线。此过程叫做 deoptimize（优化回退），将优化编译后的机器代码还原为字节码。

读到这里，你可能有些疑惑：这个假设是什么假设呢？以及为什么要优化回退？我们来看下面的例子。

```
function sum (a, b) {
    return a + b;
}
```

我们都知道 JavaScript 是基于动态类型的，a 和 b 可以是任意类型数据，当执行 sum 函数时，Ignition 解释器会检查 a 和 b 的数据类型，并相应地执行加法或者连接字符串的操作。

如果 sum 函数被调用多次，每次执行时都要检查参数的数据类型是很浪费时间的。此时 TurboFan 就出场了。它会分析监视器收集的信息，如果以前每次调用 sum 函数时传递的参数类型都是数字，那么 TurboFan 就预设 sum 的参数类型是数字类型，然后将其编译为机器指令。

但是当某一次的调用传入的参数不再是数字时，表示 TurboFan 的假设是错误的，此时优化编译生成的机器代码就不能再使用了，于是就需要进行优化回退。

#### Orinoco

Orinoco 是 V8 的垃圾回收模块（garbage collector），负责将程序不再需要的内存空间回收(标记清除法)；

### 5.编译型语言和解释型语言

所谓的二进制指令，也就是机器码，是 CPU 能够识别的硬件层面的“代码”，简陋的硬件（比如古老的单片机）只能使用几十个指令，强大的硬件（PC 和智能手机）能使用成百上千个指令。

然而，究竟在什么时候将源代码转换成二进制指令呢？不同的编程语言有不同的规定：

-   有的编程语言要求必须提前将所有源代码一次性转换成二进制指令，也就是生成一个可执行程序（Windows 下的 .exe），比如C语言、[C++](https://link.juejin.cn/?target=http%3A%2F%2Fc.biancheng.net%2Fcplus%2F)、Golang、Pascal（Delphi）、汇编等，这种编程语言称为编译型语言，使用的转换工具称为编译器。
-   有的编程语言可以一边执行一边转换，需要哪些源代码就转换哪些源代码，不会生成可执行程序，比如 [Python](https://link.juejin.cn/?target=http%3A%2F%2Fc.biancheng.net%2Fpython%2F)、[JavaScript](https://link.juejin.cn/?target=http%3A%2F%2Fc.biancheng.net%2Fjs%2F)、[PHP](https://link.juejin.cn/?target=http%3A%2F%2Fc.biancheng.net%2Fphp%2F)、Shell、[MATLAB](https://link.juejin.cn/?target=http%3A%2F%2Fc.biancheng.net%2Fmatlab%2F) 等，这种编程语言称为解释型语言，使用的转换工具称为解释器。


简单理解，`编译器就是一个“翻译工具”`，类似于将中文翻译成英文、将英文翻译成俄文。但是，翻译源代码是一个复杂的过程，大致包括词法分析、语法分析、语义分析、性能优化、生成可执行文件等五个步骤

[Java](https://link.juejin.cn/?target=http%3A%2F%2Fc.biancheng.net%2Fjava%2F) 和 [C#](https://link.juejin.cn/?target=http%3A%2F%2Fc.biancheng.net%2Fcsharp%2F) 是一种比较奇葩的存在，它们是半编译半解释型的语言，源代码需要先转换成一种中间文件（字节码文件），然后再将中间文件拿到虚拟机中执行。Java 引领了这种风潮，它的初衷是在跨平台的同时兼顾执行效率；C# 是后来的跟随者，但是 C# 一直止步于 Windows 平台，在其它平台鲜有作为。

#### 编译型语言

对于编译型语言，开发完成以后需要将所有的源代码都转换成可执行程序，比如 Windows 下的`.exe`文件，可执行程序里面包含的就是机器码。只要我们拥有可执行程序，就可以随时运行，不用再重新编译了，也就是“一次编译，无限次运行”。

在运行的时候，我们只需要编译生成的可执行程序，不再需要源代码和编译器了，所以说编译型语言可以脱离开发环境运行。

编译型语言一般是不能跨平台的，也就是不能在不同的操作系统之间随意切换。

编译型语言不能跨平台表现在两个方面：

##### 1) 可执行程序不能跨平台

可执行程序不能跨平台很容易理解，因为不同操作系统对可执行文件的内部结构有着截然不同的要求，彼此之间也不能兼容。不能跨平台是天经地义，能跨平台反而才是奇葩。

比如，不能将 Windows 下的可执行程序拿到 Linux 下使用，也不能将 Linux 下的可执行程序拿到 Mac OS 下使用（虽然它们都是[类 Unix 系统](https://link.juejin.cn/?target=http%3A%2F%2Fc.biancheng.net%2Fview%2Fvip_5038.html)）。

另外，相同操作系统的不同版本之间也不一定兼容，比如不能将 x64 程序（Windows 64 位程序）拿到 x86 平台（Windows 32 位平台）下运行。但是反之一般可行，因为 64 位 Windows 对 32 位程序作了很好的兼容性处理。

##### 2) 源代码不能跨平台

不同平台支持的函数、类型、变量等都可能不同，基于某个平台编写的源代码一般不能拿到另一个平台下编译。我们以C语言为例来说明。

【实例1】在C语言中要想让程序暂停可以使用“睡眠”函数，在 Windows 平台下该函数是 Sleep()，在 Linux 平台下该函数是 sleep()，首字母大小写不同。其次，Sleep() 的参数是毫秒，sleep() 的参数是秒，单位也不一样。

以上两个原因导致使用暂停功能的C语言程序不能跨平台，除非在代码层面做出兼容性处理，非常麻烦。

【实例2】虽然不同平台的C语言都支持 long 类型，但是不同平台的 long 的长度却不同，例如，Windows 64 位平台下的 long 占用 4 个字节，Linux 64 位平台下的 long 占用 8 个字节。

我们在 Linux 64 位平台下编写代码时，将 0x2f1e4ad23 赋值给 long 类型的变量是完全没有问题的，但是这样的赋值在 Windows 平台下就会导致数值溢出，让程序产生错误的运行结果。

让人苦恼的，这样的错误一般不容易察觉，因为编译器不会报错，我们也记不住不同类型的取值范围。

#### 解释型语言

对于解释型语言，每次执行程序都需要一边转换一边执行，用到哪些源代码就将哪些源代码转换成机器码，用不到的不进行任何处理。每次执行程序时可能使用不同的功能，这个时候需要转换的源代码也不一样。

因为每次执行程序都需要重新转换源代码，所以解释型语言的执行效率天生就低于编译型语言，甚至存在数量级的差距。计算机的一些底层功能，或者关键算法，一般都使用 C/C++ 实现，只有在应用层面（比如网站开发、批处理、小工具等）才会使用解释型语言。

在运行解释型语言的时候，我们始终都需要源代码和解释器，所以说它无法脱离开发环境。

当我们说“下载一个程序（软件）”时，不同类型的语言有不同的含义：

-   对于编译型语言，我们下载到的是可执行文件，源代码被作者保留，所以编译型语言的程序一般是闭源的。
-   对于解释型语言，我们下载到的是所有的源代码，因为作者不给源代码就没法运行，所以解释型语言的程序一般是开源的。


相比于编译型语言，解释型语言几乎都能跨平台，“一次编写，到处运行”是真是存在的，而且比比皆是。那么，为什么解释型语言就能快平台呢？

这一切都要归功于解释器！

`我们所说的跨平台，是指源代码跨平台，而不是解释器跨平台`。解释器用来将源代码转换成机器码，它就是一个可执行程序，是绝对不能跨平台的。

官方需要针对不同的平台开发不同的解释器，这些解释器必须要能够遵守同样的语法，识别同样的函数，完成同样的功能，只有这样，同样的代码在不同平台的执行结果才是相同的。

你看，`解释型语言之所以能够跨平台，是因为有了解释器这个中间层。在不同的平台下，解释器会将相同的源代码转换成不同的机器码`，解释器帮助我们屏蔽了不同平台之间的差异。

#### 总结

我们将编译型语言和解释型语言的差异总结为下表：

| 类型       | 原理                                                         | 优点                                                         | 缺点                         |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------------------------- |
| 编译型语言 | 通过专门的编译器，将所有源代码一次性转换成特定平台（Windows、Linux 等）执行的机器码（以可执行文件的形式存在）。 | 编译一次后，脱离了编译器也可以运行，并且运行效率高。         | 可移植性差，不够灵活。       |
| 解释型语言 | 由专门的解释器，根据需要将部分源代码临时转换成特定平台的机器码。 | 跨平台性好，通过不同的解释器，将相同的源代码解释成不同平台下的机器码。 | 一边执行一边转换，效率很低。 |

#### 解释器

**解释器**是一条一条的解释执行源语言(`边解释边运行`)。比如php，postscritp，javascript就是典型的解释性语言。 `运行效率低`，所以通常会进行一些**预编译的优化**。
**编译器**是把源代码整个编译成目标代码，`执行时不在需要编译器`，直接在支持目标代码的平台上运行，这样执行效率比解释`执行快`很多。比如C语言代码被编译成二进制代码（exe程序），在windows平台上执行。

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8c8967858400485f993aadb485be1269~tplv-k3u1fbpfcp-zoom-in-crop-mark:3024:0:0:0.awebp) 他们**最大的区别是程序运行时需要解释器边解释边执行，而编译器则在运行时是完全不需要的**。

解释器的优点是比较容易让用户实现自己跨平台的代码，比如java，php等，同一套代码可以在几乎所有的操作系统上执行，而无需根据操作系统做修改； 编译器的目的就是生成目标代码再由连接器生成可执行的机器码，这样的话需要根据不同的操作系统编制代码，虽然有像Qt这样的源代码级跨平台的编程工具库，但在不同的平台上仍然需要重新编译连接成可执行文件，但其执行效率要远远高于解释运行的程序



## 后端开发理论

### 1.什么是ioc和aop

#### IOC

##### 概述

IoC （Inversion of control ）控制反转/反转控制。它是一种思想不是一个技术实现。描述的是：Java 开发领域对象的创建以及管理的问题。

例如：现有类 A 依赖于类 B

-   **传统的开发方式** ：往往是在类 A 中手动通过 new 关键字来 new 一个 B 的对象出来
-   **使用 IoC 思想的开发方式** ：不通过 new 关键字来创建对象，而是通过 IoC 容器(Spring 框架) 来帮助我们实例化对象。我们需要哪个对象，直接从 IoC 容器里面过去即可。

从以上两种开发方式的对比来看：我们 “丧失了一个权力” (创建、管理对象的权力)，从而也得到了一个好处（不用再考虑对象的创建、管理等一系列的事情）

##### 为什么叫 控制反转

**控制** ：指的是对象创建（实例化、管理）的权力

**反转** ：控制权交给外部环境（Spring 框架、IoC 容器）

##### IoC 解决了什么问题

IoC 的思想就是两方之间不互相依赖，由第三方容器来管理相关资源。这样有什么好处呢？

1.  对象之间的耦合度或者说依赖程度降低；
2.  资源变的容易管理；比如你用 Spring 容器提供的话很容易就可以实现一个单例。

例如：现有一个针对 User 的操作，利用 Service 和 Dao 两层结构进行开发

在没有使用 IoC 思想的情况下，Service 层想要使用 Dao 层的具体实现的话，需要通过 new 关键字在`UserServiceImpl` 中手动 new 出 `IUserDao` 的具体实现类 `UserDaoImpl`（不能直接 new 接口类）。

开发过程中突然接到一个新的需求，针对对`IUserDao` 接口开发出另一个具体实现类。因为 Server 层依赖了`IUserDao`的具体实现，所以我们需要修改`UserServiceImpl`中 new 的对象。如果只有一个类引用了`IUserDao`的具体实现，可能觉得还好，修改起来也不是很费力气，但是如果有许许多多的地方都引用了`IUserDao`的具体实现的话，一旦需要更换`IUserDao` 的实现方式，那修改起来将会非常的头疼。

使用 IoC 的思想，我们将对象的控制权（创建、管理）交有 IoC 容器去管理，我们在使用的时候直接向 IoC 容器 “要” 就可以了

#### AOP

AOP：Aspect oriented programming 面向切面编程，AOP 是 OOP（面向对象编程）的一种延续。

下面我们先看一个 OOP 的例子。

例如：现有三个类，`Horse`、`Pig`、`Dog`，这三个类中都有 eat 和 run 两个方法。

通过 OOP 思想中的继承，我们可以提取出一个 Animal 的父类，然后将 eat 和 run 方法放入父类中，`Horse`、`Pig`、`Dog`通过继承`Animal`类即可自动获得 `eat()` 和 `run()` 方法。这样将会少些很多重复的代码。

OOP 编程思想可以解决大部分的代码重复问题。但是有一些问题是处理不了的。比如在父类 Animal 中的多个方法的相同位置出现了重复的代码，OOP 就解决不了

这部分重复的代码，一般统称为 **横切逻辑代码**。

横切逻辑代码存在的问题：

-   代码重复问题
-   横切逻辑代码和业务代码混杂在一起，代码臃肿，不变维护

AOP 另辟蹊径，提出横向抽取机制，将横切逻辑代码和业务逻辑代码分离

![img](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2020/5/28/1725a6b2d5fad550~tplv-t2oaga2asx-zoom-in-crop-mark:3024:0:0:0.awebp)img

代码拆分比较容易，难的是如何在不改变原有业务逻辑的情况下，悄无声息的将横向逻辑代码应用到原有的业务逻辑中，达到和原来一样的效果。

##### AOP 解决了什么问题

通过上面的分析可以发现，AOP 主要用来解决：在不改变原有业务逻辑的情况下，增强横切逻辑代码，根本上解耦合，避免横切逻辑代码重复。

##### AOP 为什么叫面向切面编程

**切** ：指的是横切逻辑，原有业务逻辑代码不动，只能操作横切逻辑代码，所以面向横切逻辑

**面** ：横切逻辑代码往往要影响的是很多个方法，每个方法如同一个点，多个点构成一个面。这里有一个面的概念

### 2.前后端分离中，后端负责了什么

#### 后端工作

后端专注于：后端控制层（Restful API） & 服务层 & 数据访问层；

　　前端专注于：前端控制层（Nodejs） & 视图层

　　**1、项目设计阶段**，前后端架构负责人将项目整体进行分析，讨论并确定API风格、职责分配、开发协助模式，确定人员配备；设计确定后，前后端人员共同制定开发接口。

　　**2、项目开发阶段**，前后端分离是各自分工，协同敏捷开发，后端提供Restful API，并给出详细文档说明，前端人员进行页面渲染前台的任务是发送API请(GET,PUT,POST,DELETE等)获取数据（json，xml）后渲染页面。

　　**3、项目测试阶段**，**API完成之前，前端人员会使用mock server进行模拟测试**，**后端人员采用junit进行API单元测试，不用互相等待**；API完成之后，前后端再对接测试一下就可以了，当然并不是所有的接口都可以提前定义，有一些是在开发过程中进行调整的。

　　**4、项目部署阶段**，利用nginx 做反向代理，即Java + nodejs + nginx 方式进行。

#### **后端技术发展**

   互联网，尤其是移动互联网开始兴起以后，海量的用户呼啸而来，一个单机部署的小小War包肯定是撑不住了，必须得做分布式。

​    原来的单个Tomcat得变成Tomcat的**集群**，前边弄个Web服务器做请求的**负载均衡，**不仅如此，还得考虑状态问题，session的一致性。

​    业务越来越复杂，我们不得不把**某些业务放到一个机器（或集群）**上，把另外一部分业务放到另外一个机器（或集群）上，虽然系统的计算能力，处理能力大大增强，但是**这些系统之间的通信**就变成了头疼的问题，**消息队列**（MQ)，**RPC框架**（如Dubbo）应运而生，为了提高通信效率，各种**序列化的工具**(如Protobuf)也争先空后地问世。

​    单个数据库也撑不住了，那就做数据库的**读写分离**，如果还不行，就做**分库和分表**，把原有的数据库垂直地切一切，或者水平地切一切， 但不管怎么切，都会让应用程序的访问非常麻烦，因为数据要跨库做Join/排序，还需要事务，为了解决这个问题，又有各种各样“**数据访问中间件**”的工具和产品诞生。

​    为了最大程度地提高性能，缓存肯定少不了，可以在本机做缓存(如Ehcache)，也可以做**分布式缓存**(如Redis)，如何搞**数据分片**，数据迁移，失效转移，这又是一个超级大的主题了。

​    互联网用户喜欢上传图片和文件，还得搞一个**分布式的文件系统**（如FastDFS），要求高可用，高可靠。

​    数据量大了，搜索的需求就自然而然地浮出水面，你得弄一个支持全文索引的**搜索引擎**(如Elasticsearch ,Solr)出来。

​    林子大了，什么鸟都有，必须得考虑**安全**，数据的加密/解密，签名、证书，防止SQL注入，XSS/CSRF等各种攻击。

#### 后端模式

MVC 模式把一个 web 应用分成三个层面，分别是 control 层（控制层），model 层（模型层）和 view 层（视图层）

![image-20220627155623636](https://s2.loli.net/2022/06/27/AZq4HTGJbIO3yUM.png)

MVC 模式的三个层面分别负责不同的功能。模型层负责专门的业务处理，视图层负责专门的UI绘制，而控制层则负责作为中转控制连接模型层和视图层。这样子的开发方式将一个 web 应用分成三个部分。

每个部分的代码量减少了，出现代码错误排错起来更方便。而且原先把控制视图的代码和控制逻辑的代码混合在一起，对于 UI 的调试不方便，也容易出现一些奇怪的错误（比如 java 的 servlet 中如果用页面流的方式输出 HTML 代码，不仅影响 servlet 的代码美观，HTML 代码中如果出现一些特殊符号（比如双引号）导致页面流提前结束，代码就会出错）。

这种将一个代码中不同功能进行模块拆分的方法，在软件工程中叫松耦合。从字面意思理解，就是降低功能和功能之间的耦合度，使一个功能在出错的时候不至于同时影响另一个功能。

在原本的 MVC 中，JSP 是负责动态生成 HTML 的。但在长期的编程中发现，有些页面在动态生成的 HTML 中有大部分数据是相同的，只有小部分数据是需要更新的，这时候的思路就从动态生成整个 HTML 页面转变成了使用静态页面再对小部分数据动态刷新。因此 Ajax 诞生了。Ajax 通过与服务器交互，获取需要动态生成的数据（以 json 或者 XML 进行传输），然后再通过 JavaScript 渲染在静态页面上，从而实现页面的生成。这样子后端就不再需要分配多余的资源给 JSP 动态生成页面了（每一个 JSP 其实都是一个特殊的 servlet，会占用服务器的资源）。于此同时，前后端也彻底分离，后端只需要负责使用 json 与前端进行交互即可，前端也只需要负责找后端获取数据渲染即可。使用 json 进行前后端的联系。这样的方式，是比传统 MVC 更松的耦合。

因为前后端是通过 json 的方式来进行数据联系，所以前后端的测试也变得更加简单。以往前后端要一起进行测试，现在使用 json 分离前后端，前端只需要使用静态的 json 就能对前端页面进行测试，而后端只需要测试生成的 json 是否符合要求即可。

### 3.restful api怎么进行设计

RestFul （REpresentational State Transfer）风格目标： 用来规范资源解释方式 与操作规则。

#### 传统URL资源定义弊端

-   通过的URL没有统一的规范，将动词与名字合并，且很难通过URL资源定向标记，了解具体的业务意义
-   当不规范的URL 随着在大型的项目中带来的问题是难以管理与维护
-   每个人对资源规则有独特的理解，那么真正的标准是什么，众多的规范，识别度底，最终干脆随心所欲，随后资源标签就腐化了

#### RestFul 作用

-   每一个URI代表一种资源
-   通过统一的标准风格规范来约束资源的表达方式，它结构清晰、符合标准、易于理解、扩展方便，所以正得到越来越多网站的采用
-   过四个HTTP动词，对服务器端资源进行操作，实现"表现层状态转化"

(一) 优点:

1.  它是面向资源的(名词)
2.  通过URL就知道需要什么资源
3.  通过Http Method(get/post...)就知道针对资源干什么
4.  通过Http Status Code就知道结果如何

(二) 优点解释:

  (1)通过URL就知道需要什么资源：表示Restful风格的API可以直接通过URL就可以看到需要操作的是什么资源,有语义化。

  (2)Restful风格的API是面向资源(名称)的,既URL中不会带相应的动词,针对资源的操作是通过Http Method(既:post-增、delete-删、put-改(一般是提供实体的全部信息)、patch-改(修改实体的某些属性)、get-查)来实现的。

  (3)通过Http Status Code就知道结果如何: 如常见的200(成功)、400(错误的请求参数)、500(服务器错误)等。

#### 接口规范

##### 1、动作

GET （SELECT）：从服务器检索特定资源，或资源列表。POST （CREATE）：在服务器上创建一个新的资源。PUT （UPDATE）：更新服务器上的资源，提供整个资源。PATCH （UPDATE）：更新服务器上的资源，仅提供更改的属性。DELETE （DELETE）：从服务器删除资源。

首先是四个半种动作：post、delete、put/patch、get因为put/patch只能算作一类，所以将patch归为半个。

另外还有有两个较少知名的HTTP动词：HEAD - 检索有关资源的元数据，例如数据的哈希或上次更新时间。OPTIONS - 检索关于客户端被允许对资源做什么的信息。

##### 2、路径（接口命名）

路径又称"终点"（endpoint），表示API的具体网址。

在RESTful架构中，每个网址代表一种资源（resource），所以网址中不能有动词，只能有名词，而且所用的名词往往与数据库的表格名对应。一般来说，数据库中的表都是同种记录的"集合"（collection），所以API中的名词也应该使用复数。

举例来说，有一个API提供动物园（zoo）的信息，还包括各种动物和雇员的信息，则它的路径应该设计成下面这样。

接口尽量使用名词，禁止使用动词，下面是一些例子。

```
GET         /zoos：列出所有动物园
POST        /zoos：新建一个动物园
GET         /zoos/ID：获取某个指定动物园的信息
PUT         /zoos/ID：更新某个指定动物园的信息（提供该动物园的全部信息）
PATCH       /zoos/ID：更新某个指定动物园的信息（提供该动物园的部分信息）
DELETE      /zoos/ID：删除某个动物园
GET         /zoos/ID/animals：列出某个指定动物园的所有动物
DELETE      /zoos/ID/animals/ID：删除某个指定动物园的指定动物
```

反例：

```
/getAllCars
/createNewCar
/deleteAllRedCars
```

再比如，某个URI是/posts/show/1，其中show是动词，这个URI就设计错了，正确的写法应该是/posts/1，然后用GET方法表示show。

如果某些动作是HTTP动词表示不了的，你就应该把动作做成一种资源。比如网上汇款，从账户1向账户2汇款500元，错误的URI是：

>   　　POST /accounts/1/transfer/500/to/2

正确的写法是把动词transfer改成名词transaction，资源不能是动词，但是可以是一种服务：

>   　　POST /transaction HTTP/1.1
>
>   　　Host: 127.0.0.1
>
>   　　from=1&to=2&amount=500.00

理清资源的层次结构，比如业务针对的范围是学校，那么学校会是一级资源(/school)，老师(/school/teachers)，学生(/school/students)就是二级资源。

##### 3、版本（Versioning）

应该将API的版本号放入URL。如：

```
https://api.example.com/v1/
```

另一种做法是，将版本号放在HTTP头信息中，但不如放入URL方便和直观。Github采用这种做法。

##### 4、过滤信息（Filtering）

如果记录数量很多，服务器不可能都将它们返回给用户。API应该提供参数，过滤返回结果。下面是一些常见的参数。

```
?limit=10：指定返回记录的数量
?offset=10：指定返回记录的开始位置。
?page_number=2&page_size=100：指定第几页，以及每页的记录数。
?sortby=name&order=asc：指定返回结果按照哪个属性排序，以及排序顺序。
?animal_type_id=1：指定筛选条件
参数的设计允许存在冗余，即允许API路径和URL参数偶尔有重复。比如，
GET /zoo/ID/animals 与 GET /animals?zoo_id=ID 的含义是相同的。
```



##### 5、状态码（Status Codes）

状态码范围

```
1xx 信息，请求收到，继续处理。范围保留用于底层HTTP的东西，你很可能永远也用不到。
2xx 成功，行为被成功地接受、理解和采纳
3xx 重定向，为了完成请求，必须进一步执行的动作
4xx 客户端错误，请求包含语法错误或者请求无法实现。范围保留用于响应客户端做出的错误，例如。他们提供不良数据或要求不存在的东西。这些请求应该是幂等的，而不是更改服务器的状态。
5xx 范围的状态码是保留给服务器端错误用的。这些错误常常是从底层的函数抛出来的，甚至
开发人员也通常没法处理，发送这类状态码的目的以确保客户端获得某种响应。
当收到5xx响应时，客户端不可能知道服务器的状态，所以这类状态码是要尽可能的避免。
```

服务器向用户返回的状态码和提示信息，常见的有以下一些（方括号中是该状态码对应的HTTP动词）。

```
200 OK - [GET]：服务器成功返回用户请求的数据，该操作是幂等的（Idempotent）。
201 CREATED - [POST/PUT/PATCH]：用户新建或修改数据成功。
202 Accepted - [*]：表示一个请求已经进入后台排队（异步任务）
204 NO CONTENT - [DELETE]：用户删除数据成功。
400 INVALID REQUEST - [POST/PUT/PATCH]：用户发出的请求有错误，服务器没有进行新建或修改数据的操作，该操作是幂等的。
401 Unauthorized - [*]：表示用户没有权限（令牌、用户名、密码错误）。
403 Forbidden - [*] 表示用户得到授权（与401错误相对），但是访问是被禁止的。
404 NOT FOUND - [*]：用户发出的请求针对的是不存在的记录，服务器没有进行操作，该操作是幂等的。
406 Not Acceptable - [GET]：用户请求的格式不可得（比如用户请求JSON格式，但是只有XML格式）。
410 Gone -[GET]：用户请求的资源被永久删除，且不会再得到的。
422 Unprocesable entity - [POST/PUT/PATCH] 当创建一个对象时，发生一个验证错误。
500 INTERNAL SERVER ERROR - [*]：服务器发生错误，用户将无法判断发出的请求是否成功。
502 网关错误
503 Service Unavailable
504 网关超时
```

### 4.RESTful架构与RPC架构

在`RESTful`架构中，关注点在于资源，操作资源时使用标准方法检索并操作信息片段，在`RPC`架构中，关注点在于方法，调用方法时将像调用本地方法一样调用服务器的方法。

#### RESTful架构

`REST`即表述性状态传递`Representational State Transfer`，是一种软件架构风格，也可以称作是一种设计`API`的模式，`REST`通过`HTTP`协议定义的通用动词方法`GET`、`POST`、`PUT`、`DELETE`，以`URI`对网络资源进行唯一标识，响应端根据请求端的不同需求，通过无状态通信，对其请求的资源进行表述，符合`REST`设计规范的架构就称为`RESTful`架构。

##### 主要原则

- 网络上的所有事物都被抽象为资源
- 每个资源都有一个唯一的资源标识符
- 对资源的各种操作不会改变资源标识符
- 所有的操作都是无状态的
- 同一个资源具有多种表现形式如`xml`、`json`等

##### 统一资源接口

安全性是指访问`REST`接口时不会对服务端资源状态发生改变。
幂等性是指对于同一`REST`接口的`URI`多次访问时，得到的资源状态是相同的。

- `GET`: 安全的，幂等的，用于读取资源
- `POST`: 不安全的，不幂等的，用于服务端自动产生的实例号创建资源，更新部分资源
- `PUT`: 不安全的，幂等的，用于客户端的实例号创建资源，更新资源
- `DELETE`: 不安全的，幂等的，用于客户端实例号删除资源

##### 实例

- 查询`user`，`GET https://127.0.0.1/user/1`，通过直接携带`params`查询用户
- 新增`user`，`POST https://127.0.0.1/user`，请求`body`附带用户注册信息
- 修改`user`，`PUT https://127.0.0.1/user`，请求`body`附带`userid`标识信息
- 删除`user`，`DELETE https://127.0.0.1/user`，请求`body`附带`userid`标识信息
- 通过请求头`Accept`来获取同一资源的不同形式，如`application/json`与`application/xml`等
- 若将版本号看作同一资源的不同表现形式的话，同样应该在`Accept`字段来区分版本而不是直接在`URI`中添加版本号

#### RPC架构

`RPC`即远程过程调用`Remote Procedure Call`，简单的理解是一个节点请求另一个节点提供的服务，远程过程调用，是相对于本地过程调用来说的，当调用方法时就像调用本地方法一样调用远程服务器的方法，做到了轻量、无感知通信。

##### 结构组成

- 客户端`client`：服务的调用方
- 服务端`server`：服务的提供方
- 客户端存根`client stub`：将客户端请求参数打包成网络消息，再发给服务方
- 服务端存根`server stub`：接收客户端发来的消息，将消息解包，并调用本地方法

##### 通信过程

```
客户端 
1. 将这个调用映射为Call Id
2. 将这个Call Id与参数等序列化，以二进制形式打包
3. 将序列化数据包通过网络通信发送到服务端
4. 等待服务端响应
5. 服务端调用成功并返回结果，反序列化后进行下一步操作

服务端 
1. 在本地维护一个Call Id的Map，用以保证Id与调用方法的对应
2. 等待客户端请求
3. 得到一个请求后，将数据包反序列化，得到Call Id与参数等
4. 通过Map寻找Call Id所对应的函数指针
5. 通过函数指针调用函数，并将数据包反序列化后的参数传递，得到结果
6. 将结果序列化之后通过网络通信返回到客户端

注：
此处的客户端指的是本地调用者，也可以是一台服务器
此处的服务端指的是被调用者，也可以是一台服务器
数据包通信时无论是使用socket进行TCP传输，或使用HTTP进行传输都是可行的Copy to clipboardErrorCopied
```

#### 相关比较

- 在通信协议方面来说，`RESTful`是使用`HTTP`协议进行数据传输，`RPC`一般是使用`TCP`协议数据传输，当然传输协议并不是`RPC`的重点，一般使用`TCP`协议传输是因为其效率高，使用`HTTP`协议传输是完全可行的。
- 在性能方面，`RPC`的传输效率高于`RESTful`数据传输的效率，因为`RCP`具有高效紧凑的进程通信机制，且传输数据量小，在交换大量消息时效率高。
- 在灵活度方面，`RESTful`架构的灵活度高于`RPC`架构，使用`RESTful`架构具有比较好的可读性，`RPC`在编写与调试时略显繁琐。
- 使用`RESTful`架构的接口进行数据传输可以得到多语言支持，`HTTP`协议相对更规范、更通用、更标准，对于中间件而言最先支持的几种协议都包含`RESTful`数据传输规范。
- 内部服务的相互调用推荐使用`RPC`，而对外的接口推荐使用`RESTful`，例如微服务架构模式一般就采用对内`RPC`对外`RESTful`的模式。



## 常用工具

#### CI&CD

CI/CD 是一种通过在应用开发阶段引入**自动化**来频繁向客户交付应用的方法。

CI/CD 的核心概念是**持续集成、持续交付和持续部署**。它是作为一个面向开发和运营团队的解决方案，主要针对在集成新代码时所引发的问题（**也称为：“集成地狱”**）。

CI/CD 可让持续自动化和持续监控贯穿于应用的整个生命周期（从集成和测试阶段，到交付和部署）。

这些关联的事务通常被统称为 **CI/CD 管道**，由开发和运维团队以敏捷方式协同支持。

##### CI 持续集成（Continuous Integration）

协同开发是目前主流的开发方式，也就是多位开发人员可以同时处理同一个应用的不同模块或者功能。

但是，如果企业计划在同一天，将所有开发分支代码集成在一起，最终可能会花费很多时间和进行很多重复劳动，费事费力。因为代码冲突是难以避免的。

如果开发人员本地的环境和线上不一致的话，那么这个问题就更加复杂了。

持续集成（CI）可以帮助开发者更加方便地将代码更改合并到主分支。

一旦开发人员将改动的代码合并到主分支，系统就会通过自动构建应用，并运行不同级别的自动化测试（通常是单元测试和集成测试）来验证这些更改，确保这些更改没有对应用造成破坏。

如果自动化测试发现新代码和现有代码之间存在冲突，CI 可以更加轻松地快速修复这些错误。

##### CD 持续交付（Continuous Delivery）

CI 在完成了构建、单元测试和集成测试这些自动化流程后，持续交付可以自动把已验证的代码发布到企业自己的存储库。

持续交付旨在建立一个可随时将开发环境的功能部署到生产环境的代码库。

在持续交付过程中，每个步骤都涉及到了测试自动化和代码发布自动化。

在流程结束时，运维团队可以快速、轻松地将应用部署到生产环境中。

##### CD 持续部署（Continuous Deployment）

对于一个完整、成熟的 CI/CD 管道来说，最后的阶段是持续部署。

它是作为持续交付的延伸，持续部署可以自动将应用发布到生产环境。

实际上，持续部署意味着开发人员对应用的改动，在编写完成后的几分钟内就能及时生效（前提是它通过了自动化测试）。这更加便于运营团队持续接收和整合用户反馈。

总而言之，所有这些 CI/CD 的关联步骤，都极大地降低了应用的部署风险。

不过，由于还需要编写自动化测试以适应 CI/CD 管道中的各种测试和发布阶段，因此前期工作量还是很大的。

##### Jenkins

>   `Jenkins` 是开源 CI&CD 软件领导者，提供超过 1000 个插件来支持构建、部署、自动化，满足任何项目的需要。

一句话概括：`Jenkins` 是一款以插件化的方式实现 CI/CD 的软件。

#### Nginx基础

##### Nginx是什么？

`Nginx` (engine x) 是一个**轻量级、高性能的HTTP**和**反向代理服务器**,同时也是一个**通用代理服务器**(TCP/UDP/IMAP/POP3/SMTP),最初由俄罗斯人Igor Sysoev编写。

简单的说：

-   `Nginx`是一个拥有高性能HTTP和反向代理服务器，其特点是`占用内存少`，`并发能力强`，并且在现实中，nginx的并发能力要比在同类型的网页服务器中表现要好
-   `Nginx`专为`性能优化`而开发，最重要的要求便是`性能`，且十分注重效率，有报告nginx能支持高达50000个并发连接数

##### 正向代理和反向代理

Nginx 是一个反向代理服务器，那么反向代理是什么呢？我们先看看什么叫做正向代理

**正向代理**：局域网中的电脑用户想要直接访问网络是不可行的，只能通过代理服务器（Server）来访问，这种代理服务就被称为正向代理。

就好比我们俩在一块，直接对话即可，但如果我和你分隔两地，我们要想对话，必须借助一个通讯设备（如：电话）来沟通，那么这个通讯设备就是"代理服务器"，这种行为称为“正向代理”

那么反向代理是什么呢？

**反向代理**：客户端无法感知代理，因为客户端访问网络不需要配置，只要把请求发送到反向代理服务器，由**反向代理服务器去选择目标服务器**获取数据，然后再返回到客户端，此时反向代理服务器和目标服务器对外就是一个服务器，暴露的是代理服务器地址，隐藏了真实服务器IP地址。

在正向代理中，我向你打电话，你能看到向你打电话的电话号码，由电话号码知道是我给你打的，那么此时我用`虚拟电话`给你打过去，你看到的不再是我的手机号，而是`虚拟号码`,你便不知道是我给你打的，这种行为变叫做"反向代理"。

在以上述的例子简单的说下：

-   正向代理：我通过我的手机（proxy Server）去给你打电话，相当于**我和我的手机**是一个整体，与你的手机（Server）是分开的
-   反向代理：我通过我的手机（proxy Server）通过软件转化为虚拟号码去给你打电话，此时相当于**我的手机和你的手机**是一个整体，和我是分开的

##### 负载均衡

**负载均衡**：是高可用网络基础架构的关键组件，通常用于将工作**负载分布到多个服务器**来提高网站、应用、数据库或其他服务的性能和可靠性。

如果没有负载均衡，客户端与服务端的操作通常是：**客户端请求服务端，然后服务端去数据库查询数据，将返回的数据带给客户端**：

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b03812eb40b047be8052ee9288f6798e~tplv-k3u1fbpfcp-zoom-in-crop-mark:1304:0:0:0.awebp?)

但随着客户端越来越多，数据，访问量飞速增长，这种情况显然无法满足，我们从上图发现，客户端的请求和相应都是通过服务端的，那么我们加大服务端的量，让多个服务端分担，是不是就能解决这个问题了呢？

但此时对于客户端而言，他去访问这个地址就是固定的，才不会去管那个服务端有时间，你只要给我返回出数据就OK了，所以我们就需要一个“管理者“，将这些服务端找个老大过来，客户端直接找老大，再由老大分配谁处理谁的数据，从而减轻服务端的压力，而这个”老大“就是**反向代理服务器**，而端口号就是这些服务端的工号。

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c6c634ec69f54d5ab76644d8dd78b0c2~tplv-k3u1fbpfcp-zoom-in-crop-mark:1304:0:0:0.awebp?)

像这样，当有15个请求时，反向代理服务器会平均分配给服务端，也就是各处理5个，这个过程就称之为：**负载均衡**

##### 动静分离

当客户端发起请求时，正常的情况是这样的：

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0f9e589ff2224234ad1413e4c4c58cce~tplv-k3u1fbpfcp-zoom-in-crop-mark:1304:0:0:0.awebp?)

就好比你去找客服，一般先是先说一大堆官方的话，你问什么，他都会这么说，那么这个就叫**静态资源（可以理解为是html，css）**

而回答具体的问题时，每个回答都是不同的，而这些不同的就叫做**动态资源（会改变，可以理解为是变量）**

在未分离的时候，可以理解为每个客服都要先说出官方的话，在打出具体的回答，这无异加大了客服的工作量，所以为了更好的有效利用客服的时间，我们把这些官方的话分离出来，找个机器人，让他代替客服去说，这样就减轻了客服的工作量。

也就是说，我们将动态资源和静态资源分离出来，交给不同的服务器去解析，这样就加快了解析的速度，从而降低由单个服务器的压力

![image.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a91e7a373df14e90891e6f4f62a629d7~tplv-k3u1fbpfcp-zoom-in-crop-mark:1304:0:0:0.awebp?)

## Nginx

### 安装使用

#### 一、 安装依赖包

```nginx
//一键安装上面四个依赖
yum -y install gcc zlib zlib-devel pcre-devel openssl openssl-devel
```

#### 二、 下载并解压安装包

```nginx
//创建一个文件夹
cd /usr/local
mkdir nginx
cd nginx

//下载tar包
wget http://nginx.org/download/nginx-1.13.7.tar.gz
tar -xvf nginx-1.13.7.tar.gz
```

#### 三、 安装nginx

```java
//进入nginx目录
cd /usr/local/nginx

//进入目录
cd nginx-1.13.7

//执行命令
./configure

//执行make命令
make

//执行make install命令
make install
```

#### 四、 配置nginx.conf

```nginx
# 打开配置文件
vi /usr/local/nginx/conf/nginx.conf

```

将端口号改成8089，因为可能apeache占用80端口，apeache端口尽量不要修改，我们选择修改nginx端口。

localhost修改为你服务器ip地址。

![20.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0bf9c1cfb94343ef91591d4a9622ec5a~tplv-k3u1fbpfcp-zoom-in-crop-mark:1304:0:0:0.awebp?)

#### 五、 启动nginx

```nginx
/usr/local/nginx/sbin/nginx -s reload
```

如果出现报错：`nginx: [error] open() ＂/usr/local/nginx/logs/nginx.pid＂ failed`

则运行： 

```nginx
/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf
```

再次启动即可！

#### 六、 查看nginx进程是否启动

```nginx
ps -ef | grep nginx
```

![21.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0c05736dfbc242579e8d731bcd8cdb5c~tplv-k3u1fbpfcp-zoom-in-crop-mark:1304:0:0:0.awebp?)

#### 七、 关闭虚拟机的防火墙

若想使用外部主机连接上虚拟机访问端口，需要关闭虚拟机的防火墙：

```
centOS6及以前版本使用命令： systemctl stop iptables.service

centOS7关闭防火墙命令： systemctl stop firewalld.service
```

### 把dist部署到Nginx上

我们nginx安装好了，现在就进入主题：前端仔，快把dist部署到Nginx上。

在/usr/local/nginx/conf中的server对象中设置alias配置dist的目录路径即可。

就比如我dist文件夹放在/home/tsVue/vue/dist/下;

![60.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b4b122df97cc4126a50688866c1c8f38~tplv-k3u1fbpfcp-zoom-in-crop-mark:1304:0:0:0.awebp?)

修改完后，记得重启一下nginx。  `./nginx -s reload`



### 基本概念

#### 正向代理

- 一个位于客户端和原始服务器(`origin server`)之间的服务器，为了**从原始服务器取得内容**，客户端向代理**发送一个请求并指定目标**(原始服务器)，然后**代理向原始服务器转交请求**并将获得的内容返回给客户端。

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/917bd3ec2c7c4823910c5d8a68e0cfe9~tplv-k3u1fbpfcp-zoom-in-crop-mark:1304:0:0:0.awebp?)

- 特点：
    1. **代理服务器和客户端处于同一个局域网内**；
    2. **客户端明确要访问的服务器地址**；
    3. 屏蔽或者隐藏了真实客户端信息。
- 作用：
    1. 访问原来无法访问的资源，如 `Google`；
    2. 可以做缓存，加速访问资源；
    3. 对客户端访问授权，上网进行认证；
    4. 代理可以记录用户访问记录（上网行为管理），对外隐藏用户信息。

#### 反向代理

- **运行方式**是**代理服务器接受网络上的连接请求**。它**将请求转发给内部网络上的服务器**，并将从服务器上得到的结果返回给网络上请求连接的客户端，此时代理服务器对外就表现为一个服务器。

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8ea7c4ce2b884c29b0a200ff042b4de0~tplv-k3u1fbpfcp-zoom-in-crop-mark:1304:0:0:0.awebp?)

- 特点：
    1. **代理服务器和源站则处于同一个局域网内**；
    2. **客户端是无感知代理的存在的，反向代理对外都是透明的**；
    3. 隐藏了服务器的信息。
- 作用：
    1. 保证内网的安全，通常将反向代理作为公网访问地址，`Web` 服务器是内网;；
    2. 负载均衡，通过反向代理服务器来优化网站的负载。

#### 负载均衡

- 多在高并发情况下需要使用。**其原理就是将数据流量分摊到多个服务器执行，减轻每台服务器的压力，多台服务器（集群）完成工作任务，从而提高了数据的吞吐量。**
- 负载均衡策略：轮询（默认）、加权（权重）、`ip_hash`、`url_hash`（第三方）、`fair`（第三方）

#### 动静分离

- `nginx` 提供的动静分离是指把**动态请求**和**静态请求**分离开，合适的服务器处理相应的请求，使整个服务器系统的性能、效率更高。
- `nginx` 可以根据配置对不同的请求做不同转发，这是动态分离的基础。静态请求对应的静态资源可以直接放在`nginx` 上做缓冲，更好的做法是放在相应的缓冲服务器上。动态请求由相应的后端服务器处理。
- 基本代码示例：

```
server { 
    ...
    # 所有静态请求都由nginx处理，存放目录为html 
    location ~ \.(gif|jpg|jpeg|png|bmp|swf|css|js)$ { 
        root e:\wwwroot; 
    } 
    # 所有动态请求都转发给 tomcat 处理 
    location ~ \.(jsp|do)$ { 
        proxy_pass http://test; 
    }
}
```

### nginx配置

#### 文件结构

```nginx
...       # 全局块

events {  # events块
   ...
}

http      # http块
{
    ...   # http全局块
    server                   # server块
    { 
        ...                  # server全局块
        location [PATTERN]   # location块
        {
            ...
        }
        location [PATTERN] 
        {
            ...
        }
    }
    server
    {
      ...
    }
    ...   # http全局块
}

```

全局块：配置影响 `nginx` 全局的指令。一般有运行 `nginx` 服务器的用户组，`nginx` 进程 `pid` 存放路径，日志存放路径，配置文件引入，允许生成 `worker process` 数等。

`events` 块：配置影响 `nginx` 服务器或与用户的网络连接。有每个进程的最大连接数，选取哪种事件驱动模型处理连接请求，是否允许同时接受多个网路连接，开启多个网络连接序列化等。

`http` 块：可以嵌套多个 `server` ，配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置。如文件引入，`mime-type` 定义，日志自定义，是否使用 `sendfile` 传输文件，连接超时时间，单连接请求数等。

`server` 块：配置虚拟主机的相关参数，一个 `http` 中可以有多个 `server`。

`location` 块：配置请求的路由，以及各种页面的处理情况。

### 配置文件实例

```js
########### 每个指令必须有分号结束。#################
#user administrator administrators;  #配置用户或者组，默认为 nobody nobody。
#worker_processes 2;  #允许生成的进程数，默认为1
#pid /nginx/pid/nginx.pid;   #指定nginx进程运行文件存放地址
error_log log/error.log debug;  #制定日志路径，级别。这个设置可以放入全局块，http块，server块，级别以此为：debug|info|notice|warn|error|crit|alert|emerg
events {
    accept_mutex on;   #设置网路连接序列化，防止惊群现象发生，默认为on
    multi_accept on;  #设置一个进程是否同时接受多个网络连接，默认为off
    #use epoll;      #事件驱动模型，select|poll|kqueue|epoll|resig|/dev/poll|eventport
    worker_connections  1024;    #最大连接数，默认为512
}
http {
    include       mime.types;   #文件扩展名与文件类型映射表
    default_type  application/octet-stream; #默认文件类型，默认为text/plain
    #access_log off; #取消服务日志    
    log_format myFormat '$remote_addr–$remote_user [$time_local] $request $status $body_bytes_sent $http_referer $http_user_agent $http_x_forwarded_for'; #自定义格式
    access_log log/access.log myFormat;  #combined为日志格式的默认值
    sendfile on;   #允许sendfile方式传输文件，默认为off，可以在http块，server块，location块。
    sendfile_max_chunk 100k;  #每个进程每次调用传输数量不能大于设定的值，默认为0，即不设上限。
    keepalive_timeout 65;  #连接超时时间，默认为75s，可以在http，server，location块。

    upstream mysvr {   
      server 127.0.0.1:7878;
      server 192.168.10.121:3333 backup;  #热备
    }
    error_page 404 https://www.baidu.com; #错误页
    server {
        keepalive_requests 120; #单连接请求上限次数。
        listen       4545;   #监听端口
        server_name  127.0.0.1;   #监听地址       
        location  ~*^.+$ {       #请求的url过滤，正则匹配，~为区分大小写，~*为不区分大小写。
           #root path;  #根目录
           #index vv.txt;  #设置默认页
           proxy_pass  http://mysvr;  #请求转向mysvr 定义的服务器列表
           deny 127.0.0.1;  #拒绝的ip
           allow 172.18.5.54; #允许的ip           
        } 
    }
}

```

每个指令必须有分号结束（配置编写完成后，可使用 `nginx -t` 检查是否正确）。

`$remote_addr` 与 `$http_x_forwarded_for` 用以记录客户端的 `ip` 地址； 

`$remote_user` 用来记录客户端用户名称；

`$time_local` 用来记录访问时间与时区；

`$request` 用来记录请求的url与http协议；

`$status` 用来记录请求状态（成功是 `200`）；

`$body_bytes_s ent` 记录发送给客户端文件主体内容大小；

`$http_referer` 用来记录从那个页面链接访问过来的；

`$http_user_agent` 记录客户端浏览器的相关信息。

### nginx命令

#### 帮助

```
nginx -h
```

#### 检查版本

```
nginx -v
```

#### 检查配置文件是否有效

```
nginx -t
```

#### 查看/停止进程

```
ps aux|grep nginx
sudo kill -9 [进程id]
```

#### 启动

```
nginx
nginx -c /usr/local/etc/nginx/conf/nginx.conf // 启动指定配置文件
```



#### 重启

```
nginx -s reload
```

#### 停止服务

```
nginx -s stop // 暴力停止
nginx -s quit // 优雅停止
```

## Git命令

<img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20220709094028533.png" alt="image-20220709094028533" style="zoom:200%;" />



在实际开发中，会使用git作为版本控制工具来完成团队协作。因此，对基本的git操作指令进行总结是十分有必要的，本文对一些术语或者理论基础，不重新码字，可以[参考廖雪峰老师的博文](https://link.juejin.cn/?target=https%3A%2F%2Fwww.liaoxuefeng.com%2Fwiki%2F0013739516305929606dd18361248578c67b8067c8c017b000)，本文只对命令做归纳总结。

[git总结](https://juejin.cn/post/6844903598522908686)

[工作中使用](https://juejin.cn/post/6974184935804534815)

![git操作](https://s2.loli.net/2022/04/26/Bb8wZcYeF7gOL5E.png)

### 基本使用

#### 1.基本原理和使用

 git的通用操作流程如下图（来源于网络）



![git操作通用流程](https://s2.loli.net/2022/07/09/9KpnfUhtbOEIlGu.webp)



主要涉及到四个关键点：

1.  工作区：本地电脑存放项目文件的地方，比如learnGitProject文件夹；
2.  暂存区（Index/Stage）：在使用git管理项目文件的时候，其本地的项目文件会多出一个.git的文件夹，将这个.git文件夹称之为版本库。其中.git文件夹中包含了两个部分，一个是暂存区（Index或者Stage）,顾名思义就是暂时存放文件的地方，通常使用add命令将工作区的文件添加到暂存区里；
3.  本地仓库：.git文件夹里还包括git自动创建的master分支，并且将HEAD指针指向master分支。使用commit命令可以将暂存区中的文件添加到本地仓库中；
4.  远程仓库：不是在本地仓库中，项目代码在远程git服务器上，比如项目放在github上，就是一个远程仓库，通常使用clone命令将远程仓库拷贝到本地仓库中，开发后推送到远程仓库中即可；

更细节的来看：



![git几个核心区域间的关系](https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2018/4/25/162fcc0e7e711dc7~tplv-t2oaga2asx-zoom-in-crop-mark:1304:0:0:0.awebp)



日常开发时代码实际上放置在工作区中，也就是本地的XXX.java这些文件，通过add等这些命令将代码文教提交给暂存区（Index/Stage），也就意味着代码全权交给了git进行管理，之后通过commit等命令将暂存区提交给master分支上，也就是意味打了一个版本，也可以说代码提交到了本地仓库中。另外，团队协作过程中自然而然还涉及到与远程仓库的交互。

因此，经过这样的分析，git命令可以分为这样的逻辑进行理解和记忆：

1. git管理配置的命令；

   **几个核心存储区的交互命令：**

2. 工作区与暂存区的交互；

3. 暂存区与本地仓库（分支）上的交互；

4. 本地仓库与远程仓库的交互。

#### 2. git配置命令

>   查询配置信息

1.  列出当前配置：`git config --list`;
2.  列出repository配置：`git config --local --list`;
3.  列出全局配置：`git config --global --list`;
4.  列出系统配置：`git config --system --list`;

>   第一次使用git，配置用户信息

1.  配置用户名：`git config --global user.name "your name"`;
2.  配置用户邮箱：`git config --global user.email "youremail@github.com"`;

>   其他配置

1.  配置解决冲突时使用哪种差异分析工具，比如要使用vimdiff：`git config --global merge.tool vimdiff`;
2.  配置git命令输出为彩色的：`git config --global color.ui auto`;
3.  配置git使用的文本编辑器：`git config --global core.editor vi`;

#### 3. 工作区上的操作命令

>   新建仓库

1.  将工作区中的项目文件使用git进行管理，即创建一个新的本地仓库：`git init`；
2.  从远程git仓库复制项目：`git clone <url>`，如：git clone git://github.com/wasd/example.git;克隆项目时如果想定义新的项目名，可以在clone命令后指定新的项目名：`git clone git://github.com/wasd/example.git mygit`；

>   提交

1.  提交工作区所有文件到暂存区：`git add .`
2.  提交工作区中指定文件到暂存区：`git add <file1> <file2> ...`;
3.  提交工作区中某个文件夹中所有文件到暂存区：`git add [dir]`;

>   撤销

1.  删除工作区文件，并且也从暂存区删除对应文件的记录：`git rm <file1> <file2>`;
2.  从暂存区中删除文件，但是工作区依然还有该文件:`git rm --cached <file>`;
3.  取消暂存区已经暂存的文件：`git reset HEAD <file>...`;
4.  撤销上一次对文件的操作：`git checkout --<file>`。要确定上一次对文件的修改不再需要，如果想保留上一次的修改以备以后继续工作，可以使用stashing和分支来处理；
5.  隐藏当前变更，以便能够切换分支：`git stash`；
6.  查看当前所有的储藏：`git stash list`；
7.  应用最新的储藏：`git stash apply`，如果想应用更早的储藏：`git stash apply stash@{2}`；重新应用被暂存的变更，需要加上`--index`参数：`git stash apply --index`;
8.  使用apply命令只是应用储藏，而内容仍然还在栈上，需要移除指定的储藏：`git stash drop stash{0}`；如果使用pop命令不仅可以重新应用储藏，还可以立刻从堆栈中清除：`git stash pop`;
9.  在某些情况下，你可能想应用储藏的修改，在进行了一些其他的修改后，又要取消之前所应用储藏的修改。Git没有提供类似于 stash unapply 的命令，但是可以通过取消该储藏的补丁达到同样的效果：`git stash show -p stash@{0} | git apply -R`；同样的，如果你沒有指定具体的某个储藏，Git 会选择最近的储藏：`git stash show -p | git apply -R`；

>   更新文件

1.  重命名文件，并将已改名文件提交到暂存区：`git mv [file-original] [file-renamed]`;

>   查新信息

1.  查询当前工作区所有文件的状态：`git status`;
2.  比较工作区中当前文件和暂存区之间的差异，也就是修改之后还没有暂存的内容：git diff；指定文件在工作区和暂存区上差异比较：`git diff <file-name>`;

#### 4. 暂存区上的操作命令

>   提交文件到版本库

1.  将暂存区中的文件提交到本地仓库中，即打上新版本：`git commit -m "commit_info"`;
2.  将所有已经使用git管理过的文件暂存后一并提交，跳过add到暂存区的过程：`git commit -a -m "commit_info"`;
3.  提交文件时，发现漏掉几个文件，或者注释写错了，可以撤销上一次提交：`git commit --amend`;

>   查看信息

1.  比较暂存区与上一版本的差异：`git diff --cached`;
2.  指定文件在暂存区和本地仓库的不同：`git diff <file-name> --cached`;
3.  查看提交历史：git log；参数`-p`展开每次提交的内容差异，用`-2`显示最近的两次更新，如`git log -p -2`;

>   打标签

Git 使用的标签有两种类型：**轻量级的（lightweight）和含附注的（annotated）**。轻量级标签就像是个不会变化的分支，实际上它就是个指向特定提交对象的引用。而含附注标签，实际上是存储在仓库中的一个独立对象，它有自身的校验和信息，包含着标签的名字，电子邮件地址和日期，以及标签说明，标签本身也允许使用 GNU Privacy Guard (GPG) 来签署或验证。一般我们都建议使用含附注型的标签，以便保留相关信息；当然，如果只是临时性加注标签，或者不需要旁注额外信息，用轻量级标签也没问题。

1.  列出现在所有的标签：`git tag`;
2.  使用特定的搜索模式列出符合条件的标签，例如只对1.4.2系列的版本感兴趣：`git tag -l "v1.4.2.*"`;
3.  创建一个含附注类型的标签，需要加`-a`参数，如`git tag -a v1.4 -m "my version 1.4"`;
4.  使用git show命令查看相应标签的版本信息，并连同显示打标签时的提交对象：`git show v1.4`;
5.  如果有自己的私钥，可以使用GPG来签署标签，只需要在命令中使用`-s`参数：`git tag -s v1.5 -m "my signed 1.5 tag"`;
6.  验证已签署的标签：git tag -v ，如`git tag -v v1.5`;
7.  创建一个轻量级标签的话，就直接使用git tag命令即可，连`-a`,`-s`以及`-m`选项都不需要，直接给出标签名字即可，如`git tag v1.5`;
8.  将标签推送到远程仓库中：git push origin ，如`git push origin v1.5`；
9.  将本地所有的标签全部推送到远程仓库中：`git push origin --tags`;

>   分支管理

1.  创建分支：`git branch <branch-name>`，如`git branch testing`；
2.  从当前所处的分支切换到其他分支：`git checkout <branch-name>`，如`git checkout testing`；
3.  新建并切换到新建分支上：`git checkout -b <branch-name>`;
4.  删除分支：`git branch -d <branch-name>`；
5.  将当前分支与指定分支进行合并：`git merge <branch-name>`;
6.  显示本地仓库的所有分支：`git branch`;
7.  查看各个分支最后一个提交对象的信息：`git branch -v`;
8.  查看哪些分支已经合并到当前分支：`git branch --merged`;
9.  查看当前哪些分支还没有合并到当前分支：`git branch --no-merged`;
10.  把远程分支合并到当前分支：`git merge <remote-name>/<branch-name>`，如`git merge origin/serverfix`；如果是单线的历史分支不存在任何需要解决的分歧，只是简单的将HEAD指针前移，所以这种合并过程可以称为快进（Fast forward），而如果是历史分支是分叉的，会以当前分叉的两个分支作为两个祖先，创建新的提交对象；如果在合并分支时，遇到合并冲突需要人工解决后，再才能提交；
11.  在远程分支的基础上创建新的本地分支`：git checkout -b <branch-name> <remote-name>/<branch-name>`，如`git checkout -b serverfix origin/serverfix`;
12.  从远程分支checkout出来的本地分支，称之为跟踪分支。在跟踪分支上向远程分支上推送内容：`git push`。该命令会自动判断应该向远程仓库中的哪个分支推送数据；在跟踪分支上合并远程分支：`git pull`；
13.  将一个分支里提交的改变移到基底分支上重放一遍：`git rebase <rebase-branch> <branch-name>`，如`git rebase master server`，将特性分支server提交的改变在基底分支master上重演一遍；使用rebase操作最大的好处是像在单个分支上操作的，提交的修改历史也是一根线；如果想把基于一个特性分支上的另一个特性分支变基到其他分支上，可以使用`--onto`操作：`git rebase --onto <rebase-branch> <feature branch> <sub-feature-branch>`，如`git rebase --onto master server client`；使用rebase操作应该遵循的原则是：**一旦分支中的提交对象发布到公共仓库，就千万不要对该分支进行rebase操作**；

#### 5.本地仓库上的操作

1.  查看本地仓库关联的远程仓库：`git remote`；在克隆完每个远程仓库后，远程仓库默认为`origin`;加上`-v`的参数后，会显示远程仓库的`url`地址；
2.  添加远程仓库，一般会取一个简短的别名：`git remote add [remote-name] [url]`，比如：`git remote add example git://github.com/example/example.git`;
3.  从远程仓库中抓取本地仓库中没有的更新：`git fetch [remote-name]`，如`git fetch origin`;使用fetch只是将远端数据拉到本地仓库，并不自动合并到当前工作分支，只能人工合并。如果设置了某个分支关联到远程仓库的某个分支的话，可以使用`git pull`来拉去远程分支的数据，然后将远端分支自动合并到本地仓库中的当前分支；
4.  将本地仓库某分支推送到远程仓库上：`git push [remote-name] [branch-name]`，如`git push origin master`；如果想将本地分支推送到远程仓库的不同名分支：`git push <remote-name> <local-branch>:<remote-branch>`，如`git push origin serverfix:awesomebranch`;如果想删除远程分支：`git push [romote-name] :<remote-branch>`，如`git push origin :serverfix`。这里省略了本地分支，也就相当于将空白内容推送给远程分支，就等于删掉了远程分支。
5.  查看远程仓库的详细信息：`git remote show origin`；
6.  修改某个远程仓库在本地的简称：`git remote rename [old-name] [new-name]`，如`git remote rename origin org`；
7.  移除远程仓库：`git remote rm [remote-name]`；

#### 6. 忽略文件.gitignore

一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。通常都是些自动生成的文件，比如日志文件，或者编译过程中创建的临时文件等。我们可以创建一个名为 .gitignore 的文件，列出要忽略的文件模式。如下例：

```
# 此为注释 – 将被 Git 忽略
# 忽略所有 .a 结尾的文件
*.a
# 但 lib.a 除外
!lib.a
# 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO
/TODO
# 忽略 build/ 目录下的所有文件
build/
# 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt
doc/*.txt
# 忽略 doc/ 目录下所有扩展名为 txt 的文件
doc/**/*.txt
```

### [一张脑图带你掌握Git命令](https://juejin.cn/post/6869519303864123399)

![a1d538d63559402fbcfd82d68b08061c](https://s2.loli.net/2022/07/09/MC3rlcWOsYbHIXk.png)

![b675e7bb00d24232a2338f87d85d00af](https://s2.loli.net/2022/07/09/lhVIytxH6UJ9w3N.png)

版本库👉`.git`

- 当我们使用git管理文件时，比如`git init`时，这个时候，会多一个`.git`文件，我们把这个文件称之为版本库。
- `.git文件`另外一个作用就是它在创建的时候，会自动创建master分支，并且将HEAD指针指向master分支。

工作区

- 本地项目存放文件的位置
- 可以理解成图上的workspace

暂存区 (Index/Stage)

- 顾名思义就是暂时存放文件的地方，通过是通过add命令将工作区的文件添加到缓冲区

本地仓库（Repository）

- 通常情况下，我们使用commit命令可以将暂存区的文件添加到本地仓库
- 通常而言，HEAD指针指向的就是master分支

远程仓库（Remote）

- 举个例子，当我们使用GitHub托管我们项目时，它就是一个远程仓库。
- 通常我们使用clone命令将远程仓库代码拷贝下来，本地代码更新后，通过push托送给远程仓库。

#### Git文件状态

- 通常我们需要查看一个文件的状态

```bash
git status
```

- ```
  Changes not staged for commit
  ```

  - 表示得大概就是工作区有该内容，但是缓存区没有，需要我们`git add`

- ```
  Changes to be committed
  ```

  - 一般而言，这个时候，文件放在缓存区了，我们需要`git commit`

- ```
  nothing to commit, working tree clean
  ```

  - 这个时候，我们将本地的代码推送到远端即可

#### 常见命令

#### git配置命令


![29f0c70414b14fe1986b376f7b303959](https://s2.loli.net/2022/07/09/HOEKsqhWCN7LQgG.png)

- 列出当前配置

```bash
git config --list	
```

- 列出Repository配置

```bash
git config --local --list
```

- 列出全局配置

```bash
git config --global --list
```

- 列出系统配置

```bash
git config --system --list
```

- 配置用户名

```bash
git config --global user.name "your name"
```

- 配置用户邮箱

```bash
git config --global user.email "youremail@github.com"
```

#### 分支管理

![3bff7ddbc6a145f993c0841eb81c8998](https://s2.loli.net/2022/07/09/Y1EkqMiy4TPBlR5.png)

- 查看本地分支

```bash
git branch
```

- 查看远程分支

```bash
git branch -r
```

- 查看本地和远程分支

```bash
git branch -a
```

- 从当前分支，切换到其他分支

```bash
git checkout <branch-name>
// 举个例子
git checkout feature/tiantian
```

- 创建并切换到新建分支

```bash
git checkout -b <branch-name>
// 举个例子👇
git checkout -b feature/tiantian
```

- 删除分支

```bash
git branch -d <branch-name>
// 举个例子👇
git branch -d feature/tiantian
```

- 当前分支与指定分支合并

```bash
git merge <branch-name>
// 举个例子👇
git merge feature/tiantian
```

- 查看哪些分支已经合并到当前分支

```bash
git branch --merged
```

- 查看哪些分支没有合并到当前分支

```bash
git branch --no-merged
```

- 查看各个分支最后一个提交对象的信息

```bash
git branch -v
```

- 删除远程分支

```bash
git push origin -d <branch-name>
```

- 重命名分支

```bash
git branch -m <oldbranch-name> <newbranch-name>
```

- 拉取远程分支并创建本地分支

```bash
git checkout -b 本地分支名x origin/远程分支名x

// 另外一种方式,也可以完成这个操作。
git fetch origin <branch-name>:<local-branch-name>
// fetch这个指令的话,后续会梳理
```

#### fetch指令

![6c666ec139fe4dc5a08df6b811b9803d](https://s2.loli.net/2022/07/09/yE6h2e8XpiDONZL.png)

我理解的就是将远程仓库内容更新到本地，最近与师姐开发项目过程中，使用的就是这个命令。

具体是这样子的👇

#### fetch推荐写法

```bash
git fetch origin <branch-name>:<local-branch-name>
```

- 一般而言，这个origin是远程主机名，一般默认就是origin。
- `branch-name` 你要拉取的分支
- `local-branch-name` 通常而言，就是你本地新建一个新分支，将origin下的某个分支代码下载到本地分支。

举个例子👇

```bash
git fetch origin feature/template_excellent:feature/template_layout
// 你的工作目录下，就会有feature/template_layout
// 一般情况下,我们需要做的就是在这个分支上开发新需求
// 完成代码后,我们需要做的就是上传我们的分支
```

#### fetch其他写法

- 将某个远程主机的更新，全部取回本地。

```bash
git fetch <远程主机名> 
```

- 这样子的话，取回的是所有的分支更新，如果想取回特定分支，可以指定分支名👇

```bash
git fetch <远程主机名> <分支名>
```

- 当你想将某个分支的内容取回到本地下某个分支的话，如下👇

```
git fetch origin :<local-branch-name>
// 等价于👇
git fetch origin master:<local-branch-name>
```

#### 花式撤销

[Git 误操作救命篇一： 如何将改动撤销？](https://zhuanlan.zhihu.com/p/42929114)

![f29320c710544828a494918b1ec2da05](https://s2.loli.net/2022/07/09/7Q9PXGkjYL1irdE.png)

撤销**工作区**修改

- git checkout --  

暂存区文件撤销 (不覆盖工作区)

- git reset HEAD 

版本回退

- git reset --(soft | mixed | hard )  < HEAD ~(num) > |  

- | 指令    | 作用范围                                |
  | ------- | --------------------------------------- |
  | --hard  | 回退全部，包括HEAD，index，working tree |
  | --mixed | 回退部分,包括HEAD，index                |
  | --soft  | 只回退HEAD                              |

git revert  用于回滚某一个（或多个）提交引入的更改

反转该提交引入的更改 并创建一个新的「回滚提交」记录反转更改 然后更新分支引用 使其指向该提交

#### 状态查询

- 查看状态
  - git status
- 查看历史操作记录
  - git reflog
- 查看日志
  - git log 

------

#### 文档查询

- 展示Git命令大纲
  - git help (--help)
- 展示Git命令大纲全部列表
  - git help -a
- 展示具体命令说明手册
  - git help

#### 文件暂存

![1b229cb4872e4991b33181cdad72b59d](https://s2.loli.net/2022/07/09/X5YrueEsdcwQJBt.png)

- 添加改动到stash
  - git stash save -a “message”
- 删除暂存
  - git stash drop <`stash@{ID}`>
- 查看stash列表
  - git stash list
- 删除全部缓存
  - git stash clear
- 恢复改动
  - git stash pop <`stash@{ID}`>

#### 差异比较

![c779e736198247bfb0795b50dced0814](https://s2.loli.net/2022/07/09/6Xr9atnQ17oAJwF.png)

比较工作区与缓存区

- git diff

比较缓存区与本地库最近一次commit内容

- git diff -- cached

比较工作区与本地最近一次commit内容

- git diff HEAD

比较两个commit之间差异

- git diff

#### 分支命名

![fd8abe5e5605411d8dbe5c4faa0054aa](https://s2.loli.net/2022/07/09/NCVpickoM9g6Q7O.png)

**master分支**

1. 主分支，用于部署生产环境的分支，确保稳定性。
2. master分支一般由develop以及hotfix分支合并，任何情况下都不能直接修改代码。

**develop 分支**

1. develop为开发分支，通常情况下，保存最新完成以及bug修复后的代码。
2. 开发新功能时，feature分支都是基于develop分支下创建的。

**feature分支**

1. 开发新功能，基本上以develop为基础创建feature分支。
2. 分支命名：feature/ 开头的为特性分支， 命名规则: feature/user_module、 feature/cart_module。

**这点我深有体会，我在网易，mentor就是这么教我的，**通常建一个feature分支。

**release分支**

1. release 为预上线分支，发布提测阶段，会release分支代码为基准提测。

**hotfix分支**

1. 分支命名：hotfix/ 开头的为修复分支，它的命名规则与 feature 分支类似。
2. 线上出现紧急问题时，需要及时修复，以master分支为基线，创建hotfix分支，修复完成后，需要合并到master分支和develop分支。

#### 基本操作

有了上述的基本了解后，那么我们就来看看整体的一个流程吧。

- 创建本地仓库 git init

  > git init

- 链接本地仓库与远端仓库

  > git remote add  origin 
  >
  > origin默认是远端仓库别名  url 可以是**可以使用https或者ssh的方式新建**

- 检查配置信息

  - git config --list

- Git user name 与email

  > git config --global user.name "yourname"
  >
  > git config --global user.email  "your_email"

- 生成SSH密钥

  > ssh-keygen -t rsa -C "这里换上你的邮箱"
  >
  > cd ~/.ssh 里面有一个文件名为id_rsa.pub,把里面的内容复制到git库的我的SSHKEYs中

- 常看远端仓库信息

  - git remote -v

- 远端仓库重新命名

  - git remote rename old new

- 提交到缓存区

  - git add .  全部上传到缓存区
  - git add   指定文件

- 提交到本地仓库

  - git commit -m 'some message'

- 提交远程仓库

  - git push <远程主机名> <本地分支名>:<远程分支名>

- 查看分支

  - git  branch

- 创建新分支

  - git branch 

- 切换分支

  - git checkout 

- 创建分支并切换

  - git checkout -b 

- 删除分支

  - git branch -d 

- 删除远程分支

  - git push -d  

- 切换分支

  - git checkout

### [Git 撤销操作实践之 Reset Revert Rebase](https://juejin.cn/post/6997042719138791460)

#### 数据准备

```bash
# 新建 git 环境
mkdir demo; cd demo ; git init

# 新建 reset 分支
git checkout -b dev

echo AAAAA > reset.txt
git add reset.txt
git commit -m "AAAAA"

echo BBBBB >> reset.txt
git add reset.txt
git commit -m "BBBBB"

echo CCCCC >> reset.txt
git add reset.txt
git commit -m "CCCCC"

echo DDDDD >> reset.txt
git add reset.txt
git commit -m "DDDDD"


# 此时的 git log 如下
$ git log --pretty=format:"%h %s" --graph

* d6212d8 DDDDD
* e807553 CCCCC
* 3609018 BBBBB
* a9fb808 AAAAA

# 此时的文件如下
$ cat reset.txt
AAAAA
BBBBB
CCCCC
DDDDD
```

#### git reset

该命令用于回退版本，可以指定退回某一次提交的版本，常见模式如下：

- `--mixed` 默认参数回退版本并将修改放入工作区
- `--soft` 回退版本并将修改放入暂存区
- `--hard` 回退版本并将修改丢弃

```bash
# 回退到上一版本
$ git reset HEAD^ 
# 指定文件回退到上一版本
$ git reset HEAD^ hello.php 
# 回退到指定版本
$ git  reset  3609018 
```

比如上面的 demo 仓库现在需要回到 BBBBB 的提交：

```bash
$ git reset --hard f64fbe6
HEAD is now at f64fbe6 BBBBB

$ cat reset.txt
AAAAA
BBBBB

$ git log --pretty=format:"%h %s" --graph
* f64fbe6 BBBBB
* 17a6e45 AAAAA
```





#### git revert

该命令与 reset 功能基本一致。需要注意的是此次操作之前和之后的commit和history 都会保留，并且把这次撤销作为一次最新的提交。比如实现 demo 仓库现在需要回到 BBBBB 的提交对应的操作为：

```bash
# 1. 执行撤销到的 commit
$ git revert f64fbe6
error: could not revert f64fbe6... BBBBB

# 2. 解决冲突

# 3. 提交修改
$ git commit -am "revert commit"

# 验证结果
$ cat reset.txt
AAAAA
BBBBB

$ git log --pretty=format:"%h %s" --graph
* a6fa241 revert commit
* 7dedf32 DDDDD
* a513f6c CCCCC
* f64fbe6 BBBBB
* 17a6e45 AAAAA
```

对比 `git reset` 来看 `git revert`保留了完整的提交历史，不会改变项目历史，对那些已经发布到共享仓库的提交来说这是一个安全的操作。

当然 revert 还可以指定回退的提交记录， 比如回退 BBBBB、CCCCC 两次提交：

**注意 `git diff 17a6e45..a513f6c` 左开右闭的区间。**

```bash
# 先 diff 查看对应的差异
git diff 17a6e45..a513f6c
# 回退revert
$ git reset --hard HEAD^

#  解决冲突

# 查看日志
$ git log --pretty=format:"%h %s" --graph
* 81a5d64 revert BC
* 7dedf32 DDDDD
* a513f6c CCCCC
* f64fbe6 BBBBB
* 17a6e45 AAAAA
```

#### git rebase

该命令是一个非常强大的命令，功能有很多。主要的功能是修改 git 的提交历史，常见的场景为合并提交记录、修改commit message等。

下面依旧是 git revert 中的例子撤销 BBBBB、CCCCC 两次提交：

```bash
# 先 diff 查看对应的差异
git diff 17a6e45..a513f6c
# rebase 前三次提交
$ git rebase -i HEAD~3

# 修改 commit 
d 515cfeb BBBBB
d 879218f CCCCC
pick 6a664ef DDDDD

# 解决冲突并存入暂存区间（git add reset.txt）

# 继续 rebase （弹出后默认保存既可，如需修改commit log 直接编辑）
git rebase --continue 

# 查看commit log
$ git log --pretty=format:"%h %s" --graph
* 989618e DDDDD
* 17a6e45 AAAAA
```

#### 总结

- `git reset` 最佳场景本地commit，一般是回退到指定版本。
- `git rebase` 可以新增、修改、删除 commit log，一般适用于展示完美的 commit log，方便于代码的 review。
- `git revert` 可以删除一次或多次 commit，并新增 commit log 的方式完成。适用于远程仓库的提交记录。记录所有的修改历史，方便 review commit log。

